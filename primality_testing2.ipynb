{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "primality-testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTo9ssJ1uGJl",
        "colab_type": "code",
        "outputId": "f8aa905b-bcd5-4c91-96a9-9517317b8ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kxuZNTovVNm",
        "colab_type": "code",
        "outputId": "267ad8d7-a7d5-4893-8345-7ecb9bf0ec63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "pip install -U keras"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8ACXNNluI7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/gdrive/My Drive/neural-insight/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnon4NoduDrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras import regularizers\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Activation\n",
        "from keras.initializers import glorot_normal\n",
        "from keras.utils import np_utils\n",
        "from math import ceil,floor\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z3ginUduDrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_bits=15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZZKrmgwuDr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=np.load(path+'xtrain.npy').reshape(-1,n_bits)\n",
        "y_train=np.load(path+'ytrain.npy').reshape(-1,2)\n",
        "X_test=np.load(path+'xtest.npy').reshape(-1,n_bits)\n",
        "y_test=np.load(path+'ytest.npy').reshape(-1,2)\n",
        "X_train,y_train=shuffle(X_train,y_train)\n",
        "X_test,y_test=shuffle(X_test,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4GEJnHQFTuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=np.load(path+'xtrain2.npy').reshape(-1,1)\n",
        "y_train=np.load(path+'ytrain2.npy').reshape(-1,1)\n",
        "X_test=np.load(path+'xtest2.npy').reshape(-1,1)\n",
        "y_test=np.load(path+'ytest2.npy').reshape(-1,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIaKEyxDuDr7",
        "colab_type": "text"
      },
      "source": [
        "[0,1] for prime\n",
        "[1,0] for non prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lBKsSzhuDst",
        "colab_type": "text"
      },
      "source": [
        "Precision of predicting a composite number is 1 i.e There are no false positives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YExlxClSuDtV",
        "colab_type": "code",
        "outputId": "c2329920-bc71-4270-8e11-02338b805f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256,activation='relu',input_shape=(n_bits,)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer=adam,metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_42 (Dense)             (None, 256)               4096      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 45,378\n",
            "Trainable params: 45,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84OSt3hbuDtb",
        "colab_type": "code",
        "outputId": "3e7d5434-a2ff-4bfc-8e47-a92ccd63e164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "early_stop=keras.callbacks.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=40, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
        "reduce_lr=keras.callbacks.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "model.fit(X_train,y_train,epochs=200, batch_size=64,callbacks=[reduce_lr,early_stop])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4000/4000 [==============================] - 0s 117us/step - loss: 0.5035 - accuracy: 0.7728\n",
            "Epoch 2/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.3727 - accuracy: 0.8508\n",
            "Epoch 3/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3450 - accuracy: 0.8645\n",
            "Epoch 4/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.3311 - accuracy: 0.8662\n",
            "Epoch 5/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3261 - accuracy: 0.8708\n",
            "Epoch 6/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.3182 - accuracy: 0.8737\n",
            "Epoch 7/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.3121 - accuracy: 0.8737\n",
            "Epoch 8/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3114 - accuracy: 0.8748\n",
            "Epoch 9/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3092 - accuracy: 0.8773\n",
            "Epoch 10/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.3072 - accuracy: 0.8798\n",
            "Epoch 11/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.3092 - accuracy: 0.8780\n",
            "Epoch 12/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3042 - accuracy: 0.8790\n",
            "Epoch 13/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.3043 - accuracy: 0.8785\n",
            "Epoch 14/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.3034 - accuracy: 0.8783\n",
            "Epoch 15/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.3015 - accuracy: 0.8790\n",
            "Epoch 16/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.3001 - accuracy: 0.8795\n",
            "Epoch 17/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.3007 - accuracy: 0.8792\n",
            "Epoch 18/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.3014 - accuracy: 0.8777\n",
            "Epoch 19/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2974 - accuracy: 0.8790\n",
            "Epoch 20/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.3021 - accuracy: 0.8780\n",
            "Epoch 21/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2936 - accuracy: 0.8802\n",
            "Epoch 22/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2951 - accuracy: 0.8805\n",
            "Epoch 23/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2942 - accuracy: 0.8798\n",
            "Epoch 24/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2924 - accuracy: 0.8798\n",
            "Epoch 25/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2942 - accuracy: 0.8805\n",
            "Epoch 26/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2929 - accuracy: 0.8808\n",
            "Epoch 27/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2921 - accuracy: 0.8792\n",
            "Epoch 28/200\n",
            "4000/4000 [==============================] - 0s 82us/step - loss: 0.2924 - accuracy: 0.8808\n",
            "Epoch 29/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2894 - accuracy: 0.8808\n",
            "Epoch 30/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2956 - accuracy: 0.8783\n",
            "Epoch 31/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2920 - accuracy: 0.8783\n",
            "Epoch 32/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2924 - accuracy: 0.8808\n",
            "Epoch 33/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2917 - accuracy: 0.8810\n",
            "Epoch 34/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2899 - accuracy: 0.8808\n",
            "Epoch 35/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2924 - accuracy: 0.8798\n",
            "Epoch 36/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2936 - accuracy: 0.8802\n",
            "Epoch 37/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2870 - accuracy: 0.8805\n",
            "Epoch 38/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2889 - accuracy: 0.8805\n",
            "Epoch 39/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2895 - accuracy: 0.8815\n",
            "Epoch 40/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2897 - accuracy: 0.8817\n",
            "Epoch 41/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2896 - accuracy: 0.8802\n",
            "Epoch 42/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2887 - accuracy: 0.8815\n",
            "Epoch 43/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2884 - accuracy: 0.8810\n",
            "Epoch 44/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2868 - accuracy: 0.8802\n",
            "Epoch 45/200\n",
            "4000/4000 [==============================] - 0s 82us/step - loss: 0.2921 - accuracy: 0.8795\n",
            "Epoch 46/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2888 - accuracy: 0.8805\n",
            "Epoch 47/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2875 - accuracy: 0.8810\n",
            "Epoch 48/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.2894 - accuracy: 0.8798\n",
            "Epoch 49/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2897 - accuracy: 0.8810\n",
            "Epoch 50/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2875 - accuracy: 0.8820\n",
            "Epoch 51/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2875 - accuracy: 0.8800\n",
            "Epoch 52/200\n",
            "4000/4000 [==============================] - 0s 86us/step - loss: 0.2879 - accuracy: 0.8800\n",
            "Epoch 53/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2884 - accuracy: 0.8795\n",
            "Epoch 54/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.2861 - accuracy: 0.8810\n",
            "Epoch 55/200\n",
            "4000/4000 [==============================] - 0s 82us/step - loss: 0.2835 - accuracy: 0.8832\n",
            "Epoch 56/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2878 - accuracy: 0.8813\n",
            "Epoch 57/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2871 - accuracy: 0.8808\n",
            "Epoch 58/200\n",
            "4000/4000 [==============================] - 0s 82us/step - loss: 0.2870 - accuracy: 0.8820\n",
            "Epoch 59/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2847 - accuracy: 0.8815\n",
            "Epoch 60/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2831 - accuracy: 0.8825\n",
            "Epoch 61/200\n",
            "4000/4000 [==============================] - 0s 82us/step - loss: 0.2901 - accuracy: 0.8805\n",
            "Epoch 62/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2858 - accuracy: 0.8810\n",
            "Epoch 63/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2839 - accuracy: 0.8820\n",
            "Epoch 64/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2871 - accuracy: 0.8810\n",
            "Epoch 65/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2848 - accuracy: 0.8820\n",
            "Epoch 66/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2844 - accuracy: 0.8805\n",
            "Epoch 67/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.2813 - accuracy: 0.8823\n",
            "Epoch 68/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2835 - accuracy: 0.8823\n",
            "Epoch 69/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2821 - accuracy: 0.8815\n",
            "Epoch 70/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2851 - accuracy: 0.8823\n",
            "Epoch 71/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2824 - accuracy: 0.8823\n",
            "Epoch 72/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2849 - accuracy: 0.8827\n",
            "Epoch 73/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2852 - accuracy: 0.8815\n",
            "Epoch 74/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2838 - accuracy: 0.8830\n",
            "Epoch 75/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2823 - accuracy: 0.8830\n",
            "Epoch 76/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2857 - accuracy: 0.8810\n",
            "Epoch 77/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2841 - accuracy: 0.8810\n",
            "Epoch 78/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2814 - accuracy: 0.8815\n",
            "Epoch 79/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2811 - accuracy: 0.8815\n",
            "Epoch 80/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2844 - accuracy: 0.8815\n",
            "Epoch 81/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2820 - accuracy: 0.8823\n",
            "Epoch 82/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2818 - accuracy: 0.8817\n",
            "Epoch 83/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2826 - accuracy: 0.8827\n",
            "Epoch 84/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.2842 - accuracy: 0.8798\n",
            "Epoch 85/200\n",
            "4000/4000 [==============================] - 0s 71us/step - loss: 0.2839 - accuracy: 0.8805\n",
            "Epoch 86/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2821 - accuracy: 0.8827\n",
            "Epoch 87/200\n",
            "4000/4000 [==============================] - 0s 83us/step - loss: 0.2828 - accuracy: 0.8817\n",
            "Epoch 88/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2789 - accuracy: 0.8810\n",
            "Epoch 89/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2844 - accuracy: 0.8792\n",
            "Epoch 90/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2824 - accuracy: 0.8842\n",
            "Epoch 91/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2824 - accuracy: 0.8802\n",
            "Epoch 92/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2810 - accuracy: 0.8827\n",
            "Epoch 93/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2858 - accuracy: 0.8810\n",
            "Epoch 94/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2857 - accuracy: 0.8820\n",
            "Epoch 95/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2817 - accuracy: 0.8825\n",
            "Epoch 96/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2836 - accuracy: 0.8817\n",
            "Epoch 97/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2837 - accuracy: 0.8827\n",
            "Epoch 98/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2820 - accuracy: 0.8823\n",
            "Epoch 99/200\n",
            "4000/4000 [==============================] - 0s 73us/step - loss: 0.2805 - accuracy: 0.8823\n",
            "Epoch 100/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2834 - accuracy: 0.8827\n",
            "Epoch 101/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2825 - accuracy: 0.8823\n",
            "Epoch 102/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2800 - accuracy: 0.8820\n",
            "Epoch 103/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2816 - accuracy: 0.8825\n",
            "Epoch 104/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2794 - accuracy: 0.8842\n",
            "Epoch 105/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2851 - accuracy: 0.8802\n",
            "Epoch 106/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2841 - accuracy: 0.8823\n",
            "Epoch 107/200\n",
            "4000/4000 [==============================] - 0s 81us/step - loss: 0.2849 - accuracy: 0.8802\n",
            "Epoch 108/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2865 - accuracy: 0.8798\n",
            "Epoch 109/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2852 - accuracy: 0.8810\n",
            "Epoch 110/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2811 - accuracy: 0.8830\n",
            "Epoch 111/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2830 - accuracy: 0.8815\n",
            "Epoch 112/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2815 - accuracy: 0.8840\n",
            "Epoch 113/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2834 - accuracy: 0.8832\n",
            "Epoch 114/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2844 - accuracy: 0.8820\n",
            "Epoch 115/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2880 - accuracy: 0.8808\n",
            "Epoch 116/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2851 - accuracy: 0.8800\n",
            "Epoch 117/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2875 - accuracy: 0.8805\n",
            "Epoch 118/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2829 - accuracy: 0.8823\n",
            "Epoch 119/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2816 - accuracy: 0.8825\n",
            "Epoch 120/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2809 - accuracy: 0.8832\n",
            "Epoch 121/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2859 - accuracy: 0.8802\n",
            "Epoch 122/200\n",
            "4000/4000 [==============================] - 0s 75us/step - loss: 0.2821 - accuracy: 0.8827\n",
            "Epoch 123/200\n",
            "4000/4000 [==============================] - 0s 78us/step - loss: 0.2835 - accuracy: 0.8832\n",
            "Epoch 124/200\n",
            "4000/4000 [==============================] - 0s 77us/step - loss: 0.2839 - accuracy: 0.8825\n",
            "Epoch 125/200\n",
            "4000/4000 [==============================] - 0s 76us/step - loss: 0.2850 - accuracy: 0.8805\n",
            "Epoch 126/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2820 - accuracy: 0.8810\n",
            "Epoch 127/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2818 - accuracy: 0.8817\n",
            "Epoch 128/200\n",
            "4000/4000 [==============================] - 0s 74us/step - loss: 0.2834 - accuracy: 0.8815\n",
            "Epoch 129/200\n",
            "4000/4000 [==============================] - 0s 79us/step - loss: 0.2838 - accuracy: 0.8810\n",
            "Epoch 130/200\n",
            "4000/4000 [==============================] - 0s 80us/step - loss: 0.2833 - accuracy: 0.8817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9d89c6fe10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8uhrgxBuDti",
        "colab_type": "code",
        "outputId": "25cc550e-61fd-459b-e896-f600d2bf49b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_train,y_train)[1]"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000/4000 [==============================] - 0s 67us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8817499876022339"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H64q53fCIhW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "362f60f5-15ec-4a88-a110-d534f17b82e6"
      },
      "source": [
        "pred = model.predict_classes(X_test, verbose=0)\n",
        "print(classification_report([np.where(r==1)[0][0] for r in y_test], pred))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.76      0.55       500\n",
            "           1       0.02      0.00      0.01       500\n",
            "\n",
            "    accuracy                           0.38      1000\n",
            "   macro avg       0.22      0.38      0.28      1000\n",
            "weighted avg       0.22      0.38      0.28      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ4fveUHJE0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "47259498-2b69-482a-ee5e-95c993b2dd02"
      },
      "source": [
        "print(X_test,pred)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " [0 1 1 ... 1 1 0]\n",
            " ...\n",
            " [1 0 1 ... 0 1 1]\n",
            " [0 1 1 ... 0 0 1]\n",
            " [1 1 1 ... 1 0 0]] [1 0 1 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjWTgJVIJnw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bool2int(x):\n",
        "    y = 0\n",
        "    for i,j in enumerate(x):\n",
        "        y += j<<i\n",
        "    return y\n",
        "nums=[bool2int(x[::-1]) for x in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UTzs6yvJ5SW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce06f3e8-c263-42b0-93c7-761bd7426184"
      },
      "source": [
        "for i in range(1000):\n",
        "  print(nums[i],pred[i],y_test[i])"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17337 0 [1 0]\n",
            "21191 0 [0 1]\n",
            "23814 0 [1 0]\n",
            "20873 0 [0 1]\n",
            "20129 0 [0 1]\n",
            "20177 1 [0 1]\n",
            "18367 0 [0 1]\n",
            "20233 1 [0 1]\n",
            "14270 0 [1 0]\n",
            "17982 0 [1 0]\n",
            "20815 0 [1 0]\n",
            "30195 0 [1 0]\n",
            "13825 0 [1 0]\n",
            "21757 0 [0 1]\n",
            "1004 1 [1 0]\n",
            "3528 0 [1 0]\n",
            "32340 0 [1 0]\n",
            "20117 0 [0 1]\n",
            "23796 1 [1 0]\n",
            "20593 0 [0 1]\n",
            "3780 0 [1 0]\n",
            "5709 0 [1 0]\n",
            "21557 0 [0 1]\n",
            "18041 0 [0 1]\n",
            "19717 1 [0 1]\n",
            "30372 0 [1 0]\n",
            "22193 0 [0 1]\n",
            "735 0 [1 0]\n",
            "29911 0 [1 0]\n",
            "20407 0 [0 1]\n",
            "18253 0 [0 1]\n",
            "20717 0 [0 1]\n",
            "3794 0 [1 0]\n",
            "21813 0 [1 0]\n",
            "19894 0 [1 0]\n",
            "22164 0 [1 0]\n",
            "21491 0 [0 1]\n",
            "12314 0 [1 0]\n",
            "9648 0 [1 0]\n",
            "20641 0 [0 1]\n",
            "2679 0 [1 0]\n",
            "18919 1 [0 1]\n",
            "7298 0 [1 0]\n",
            "20599 0 [0 1]\n",
            "12661 0 [1 0]\n",
            "24425 0 [1 0]\n",
            "21701 0 [0 1]\n",
            "32556 0 [1 0]\n",
            "22129 0 [0 1]\n",
            "22123 0 [0 1]\n",
            "19571 1 [0 1]\n",
            "19763 0 [0 1]\n",
            "10816 0 [1 0]\n",
            "19031 0 [0 1]\n",
            "19973 0 [0 1]\n",
            "27473 0 [1 0]\n",
            "18660 0 [1 0]\n",
            "20161 1 [0 1]\n",
            "18476 0 [1 0]\n",
            "16667 0 [1 0]\n",
            "18869 0 [0 1]\n",
            "15893 0 [1 0]\n",
            "20816 0 [1 0]\n",
            "17881 0 [0 1]\n",
            "19231 0 [0 1]\n",
            "6261 0 [1 0]\n",
            "22003 0 [0 1]\n",
            "18701 0 [0 1]\n",
            "18301 0 [0 1]\n",
            "20983 1 [0 1]\n",
            "16777 0 [1 0]\n",
            "15784 0 [1 0]\n",
            "20963 0 [0 1]\n",
            "21503 0 [0 1]\n",
            "17681 0 [0 1]\n",
            "17791 0 [0 1]\n",
            "22010 0 [1 0]\n",
            "21821 0 [0 1]\n",
            "19553 0 [0 1]\n",
            "18427 0 [0 1]\n",
            "3991 0 [1 0]\n",
            "19001 0 [0 1]\n",
            "19069 0 [0 1]\n",
            "17623 0 [0 1]\n",
            "18973 0 [0 1]\n",
            "2449 1 [1 0]\n",
            "19483 0 [0 1]\n",
            "28955 0 [1 0]\n",
            "9776 0 [1 0]\n",
            "20424 0 [1 0]\n",
            "22037 0 [0 1]\n",
            "20773 0 [0 1]\n",
            "19889 1 [0 1]\n",
            "20693 0 [0 1]\n",
            "16473 0 [1 0]\n",
            "21929 0 [0 1]\n",
            "18313 1 [0 1]\n",
            "21659 1 [1 0]\n",
            "7641 1 [1 0]\n",
            "18751 0 [1 0]\n",
            "25260 1 [1 0]\n",
            "24372 0 [1 0]\n",
            "30621 0 [1 0]\n",
            "17602 0 [1 0]\n",
            "3497 1 [1 0]\n",
            "23329 0 [1 0]\n",
            "19374 0 [1 0]\n",
            "6444 0 [1 0]\n",
            "3743 0 [1 0]\n",
            "18917 0 [0 1]\n",
            "29550 1 [1 0]\n",
            "18397 0 [0 1]\n",
            "25344 0 [1 0]\n",
            "20981 0 [0 1]\n",
            "21383 0 [0 1]\n",
            "14465 0 [1 0]\n",
            "25198 0 [1 0]\n",
            "19679 0 [1 0]\n",
            "8449 0 [1 0]\n",
            "10550 0 [1 0]\n",
            "21149 0 [0 1]\n",
            "21880 0 [1 0]\n",
            "21977 0 [0 1]\n",
            "20809 0 [0 1]\n",
            "27964 0 [1 0]\n",
            "19507 0 [0 1]\n",
            "17467 0 [0 1]\n",
            "18947 0 [0 1]\n",
            "20563 0 [0 1]\n",
            "8367 0 [1 0]\n",
            "28186 0 [1 0]\n",
            "21863 0 [0 1]\n",
            "31364 0 [1 0]\n",
            "20357 0 [0 1]\n",
            "17449 0 [0 1]\n",
            "24648 0 [1 0]\n",
            "19603 0 [0 1]\n",
            "21523 0 [0 1]\n",
            "8842 0 [1 0]\n",
            "14527 0 [1 0]\n",
            "20143 1 [0 1]\n",
            "4300 0 [1 0]\n",
            "26994 0 [1 0]\n",
            "6485 0 [1 0]\n",
            "28154 0 [1 0]\n",
            "21569 0 [0 1]\n",
            "7288 0 [1 0]\n",
            "18143 1 [0 1]\n",
            "29461 0 [1 0]\n",
            "22387 0 [1 0]\n",
            "18481 0 [0 1]\n",
            "19051 0 [0 1]\n",
            "8128 0 [1 0]\n",
            "10697 1 [1 0]\n",
            "17071 0 [1 0]\n",
            "31300 0 [1 0]\n",
            "17669 1 [0 1]\n",
            "18793 0 [0 1]\n",
            "18679 0 [0 1]\n",
            "25077 0 [1 0]\n",
            "5961 0 [1 0]\n",
            "20051 1 [0 1]\n",
            "26871 0 [1 0]\n",
            "21839 1 [0 1]\n",
            "25416 0 [1 0]\n",
            "30416 0 [1 0]\n",
            "27550 0 [1 0]\n",
            "13662 1 [1 0]\n",
            "27298 0 [1 0]\n",
            "17747 0 [0 1]\n",
            "31380 0 [1 0]\n",
            "20261 0 [0 1]\n",
            "19489 0 [0 1]\n",
            "2466 1 [1 0]\n",
            "19697 1 [0 1]\n",
            "19160 0 [1 0]\n",
            "20857 0 [0 1]\n",
            "26348 0 [1 0]\n",
            "20804 0 [1 0]\n",
            "17939 0 [0 1]\n",
            "18787 0 [0 1]\n",
            "7166 0 [1 0]\n",
            "12156 0 [1 0]\n",
            "19997 0 [0 1]\n",
            "26188 1 [1 0]\n",
            "21101 0 [0 1]\n",
            "20972 0 [1 0]\n",
            "27097 1 [1 0]\n",
            "17489 0 [0 1]\n",
            "26602 0 [1 0]\n",
            "20341 0 [0 1]\n",
            "27114 0 [1 0]\n",
            "31210 0 [1 0]\n",
            "21871 0 [0 1]\n",
            "10032 0 [1 0]\n",
            "21067 1 [0 1]\n",
            "14916 0 [1 0]\n",
            "17923 0 [0 1]\n",
            "22015 0 [1 0]\n",
            "8682 0 [1 0]\n",
            "21487 0 [0 1]\n",
            "32661 0 [1 0]\n",
            "26752 0 [1 0]\n",
            "19046 1 [1 0]\n",
            "17662 0 [1 0]\n",
            "21617 0 [0 1]\n",
            "15744 0 [1 0]\n",
            "19267 0 [0 1]\n",
            "1705 0 [1 0]\n",
            "6366 0 [1 0]\n",
            "17609 0 [0 1]\n",
            "18061 0 [0 1]\n",
            "27245 0 [1 0]\n",
            "22125 0 [1 0]\n",
            "20013 0 [1 0]\n",
            "9301 0 [1 0]\n",
            "21859 0 [0 1]\n",
            "20807 0 [0 1]\n",
            "20719 0 [0 1]\n",
            "23135 0 [1 0]\n",
            "5337 0 [1 0]\n",
            "26965 0 [1 0]\n",
            "25035 0 [1 0]\n",
            "19979 0 [0 1]\n",
            "20029 0 [0 1]\n",
            "17393 0 [0 1]\n",
            "31866 0 [1 0]\n",
            "29447 0 [1 0]\n",
            "4618 0 [1 0]\n",
            "19417 0 [0 1]\n",
            "21143 0 [0 1]\n",
            "18233 0 [0 1]\n",
            "2421 0 [1 0]\n",
            "12835 0 [1 0]\n",
            "10751 0 [1 0]\n",
            "19577 0 [0 1]\n",
            "15353 1 [1 0]\n",
            "19841 1 [0 1]\n",
            "22035 0 [1 0]\n",
            "22039 0 [0 1]\n",
            "17989 1 [0 1]\n",
            "21107 1 [0 1]\n",
            "14532 1 [1 0]\n",
            "19927 0 [0 1]\n",
            "19961 0 [0 1]\n",
            "4228 0 [1 0]\n",
            "32520 0 [1 0]\n",
            "21529 0 [0 1]\n",
            "18541 0 [0 1]\n",
            "20089 0 [0 1]\n",
            "4737 0 [1 0]\n",
            "6105 0 [1 0]\n",
            "7569 0 [1 0]\n",
            "9748 0 [1 0]\n",
            "28665 0 [1 0]\n",
            "18839 0 [0 1]\n",
            "19751 0 [0 1]\n",
            "18217 0 [0 1]\n",
            "20359 0 [0 1]\n",
            "1862 0 [1 0]\n",
            "10016 0 [1 0]\n",
            "15491 0 [1 0]\n",
            "21991 1 [0 1]\n",
            "20021 0 [0 1]\n",
            "27193 0 [1 0]\n",
            "20231 0 [0 1]\n",
            "19753 0 [0 1]\n",
            "32127 0 [1 0]\n",
            "16683 0 [1 0]\n",
            "6202 0 [1 0]\n",
            "9556 0 [1 0]\n",
            "23028 0 [1 0]\n",
            "20399 0 [0 1]\n",
            "22780 0 [1 0]\n",
            "23863 0 [1 0]\n",
            "3756 0 [1 0]\n",
            "13320 0 [1 0]\n",
            "7263 0 [1 0]\n",
            "18013 0 [0 1]\n",
            "27377 1 [1 0]\n",
            "18257 0 [0 1]\n",
            "18131 0 [0 1]\n",
            "3038 0 [1 0]\n",
            "30513 0 [1 0]\n",
            "9609 0 [1 0]\n",
            "17971 0 [0 1]\n",
            "10731 0 [1 0]\n",
            "19949 0 [0 1]\n",
            "20533 0 [0 1]\n",
            "19709 0 [0 1]\n",
            "3524 0 [1 0]\n",
            "21017 0 [0 1]\n",
            "21817 0 [0 1]\n",
            "22662 0 [1 0]\n",
            "18773 0 [0 1]\n",
            "19843 0 [0 1]\n",
            "16151 0 [1 0]\n",
            "21727 0 [0 1]\n",
            "21881 0 [0 1]\n",
            "2157 0 [1 0]\n",
            "21347 1 [0 1]\n",
            "20113 0 [0 1]\n",
            "1486 0 [1 0]\n",
            "20286 0 [1 0]\n",
            "7871 0 [1 0]\n",
            "12796 0 [1 0]\n",
            "20903 0 [0 1]\n",
            "10744 1 [1 0]\n",
            "3658 1 [1 0]\n",
            "3503 0 [1 0]\n",
            "8757 0 [1 0]\n",
            "18531 0 [1 0]\n",
            "22247 0 [0 1]\n",
            "19183 1 [0 1]\n",
            "18119 0 [0 1]\n",
            "3330 0 [1 0]\n",
            "18049 0 [0 1]\n",
            "15358 1 [1 0]\n",
            "23463 0 [1 0]\n",
            "13062 0 [1 0]\n",
            "13646 0 [1 0]\n",
            "19739 0 [0 1]\n",
            "21499 0 [0 1]\n",
            "19391 0 [0 1]\n",
            "19471 0 [0 1]\n",
            "18133 0 [0 1]\n",
            "20722 0 [1 0]\n",
            "20681 1 [0 1]\n",
            "19678 1 [1 0]\n",
            "29158 0 [1 0]\n",
            "29414 0 [1 0]\n",
            "21673 0 [0 1]\n",
            "21521 0 [0 1]\n",
            "13156 1 [1 0]\n",
            "19121 0 [0 1]\n",
            "2570 0 [1 0]\n",
            "30374 0 [1 0]\n",
            "18149 0 [0 1]\n",
            "21433 0 [0 1]\n",
            "20521 0 [0 1]\n",
            "7764 0 [1 0]\n",
            "18517 0 [0 1]\n",
            "21587 0 [0 1]\n",
            "19463 1 [0 1]\n",
            "19309 0 [0 1]\n",
            "3048 0 [1 0]\n",
            "18211 0 [0 1]\n",
            "21828 0 [1 0]\n",
            "14271 0 [1 0]\n",
            "12498 0 [1 0]\n",
            "29350 0 [1 0]\n",
            "17581 1 [0 1]\n",
            "18521 1 [0 1]\n",
            "21157 0 [0 1]\n",
            "19699 0 [0 1]\n",
            "19237 0 [0 1]\n",
            "4685 0 [1 0]\n",
            "19559 0 [0 1]\n",
            "15156 0 [1 0]\n",
            "17773 0 [1 0]\n",
            "22288 0 [1 0]\n",
            "14119 0 [1 0]\n",
            "20369 0 [0 1]\n",
            "18553 0 [0 1]\n",
            "22027 0 [0 1]\n",
            "18329 0 [0 1]\n",
            "18047 0 [0 1]\n",
            "17573 0 [0 1]\n",
            "20149 0 [0 1]\n",
            "23496 0 [1 0]\n",
            "11924 0 [1 0]\n",
            "21031 0 [0 1]\n",
            "22189 0 [0 1]\n",
            "20789 0 [0 1]\n",
            "20627 0 [0 1]\n",
            "19085 0 [1 0]\n",
            "19429 1 [0 1]\n",
            "6216 0 [1 0]\n",
            "19273 0 [0 1]\n",
            "21323 0 [0 1]\n",
            "9369 0 [1 0]\n",
            "19259 0 [0 1]\n",
            "30520 0 [1 0]\n",
            "18913 1 [0 1]\n",
            "17981 0 [0 1]\n",
            "20939 0 [0 1]\n",
            "17238 0 [1 0]\n",
            "18371 0 [0 1]\n",
            "29326 0 [1 0]\n",
            "7519 0 [1 0]\n",
            "17863 0 [0 1]\n",
            "19543 0 [0 1]\n",
            "20483 0 [0 1]\n",
            "16675 0 [1 0]\n",
            "19249 0 [0 1]\n",
            "9359 0 [1 0]\n",
            "19017 0 [1 0]\n",
            "18289 1 [0 1]\n",
            "19993 0 [0 1]\n",
            "28108 0 [1 0]\n",
            "1968 0 [1 0]\n",
            "24306 0 [1 0]\n",
            "24451 0 [1 0]\n",
            "17839 0 [0 1]\n",
            "21400 0 [1 0]\n",
            "28956 0 [1 0]\n",
            "21187 0 [0 1]\n",
            "18097 0 [0 1]\n",
            "13379 0 [1 0]\n",
            "2835 1 [1 0]\n",
            "6913 0 [1 0]\n",
            "17506 0 [1 0]\n",
            "17497 0 [0 1]\n",
            "12210 0 [1 0]\n",
            "20707 0 [0 1]\n",
            "5482 0 [1 0]\n",
            "19727 0 [0 1]\n",
            "19062 0 [1 0]\n",
            "18251 0 [0 1]\n",
            "17807 0 [0 1]\n",
            "29002 0 [1 0]\n",
            "26247 0 [1 0]\n",
            "21313 0 [0 1]\n",
            "17827 0 [0 1]\n",
            "14636 0 [1 0]\n",
            "13634 0 [1 0]\n",
            "17977 0 [0 1]\n",
            "19583 0 [0 1]\n",
            "15893 0 [1 0]\n",
            "24338 0 [1 0]\n",
            "18583 0 [0 1]\n",
            "24347 0 [1 0]\n",
            "8155 1 [1 0]\n",
            "7746 0 [1 0]\n",
            "19759 0 [0 1]\n",
            "11143 0 [1 0]\n",
            "19042 1 [1 0]\n",
            "21270 0 [1 0]\n",
            "21163 0 [0 1]\n",
            "5567 0 [1 0]\n",
            "11741 0 [1 0]\n",
            "12108 0 [1 0]\n",
            "21013 1 [0 1]\n",
            "21649 0 [0 1]\n",
            "21144 0 [1 0]\n",
            "2778 0 [1 0]\n",
            "15128 0 [1 0]\n",
            "19861 0 [0 1]\n",
            "21577 0 [0 1]\n",
            "19541 1 [0 1]\n",
            "18127 0 [0 1]\n",
            "18269 0 [0 1]\n",
            "11474 0 [1 0]\n",
            "21181 0 [1 0]\n",
            "6212 0 [1 0]\n",
            "20173 0 [0 1]\n",
            "19319 0 [0 1]\n",
            "27615 0 [1 0]\n",
            "17599 0 [0 1]\n",
            "20107 0 [0 1]\n",
            "22073 1 [0 1]\n",
            "21019 0 [0 1]\n",
            "19793 0 [0 1]\n",
            "21277 0 [0 1]\n",
            "21609 0 [1 0]\n",
            "21317 0 [0 1]\n",
            "13324 0 [1 0]\n",
            "31857 0 [1 0]\n",
            "10849 0 [1 0]\n",
            "21001 0 [0 1]\n",
            "19037 1 [0 1]\n",
            "10260 0 [1 0]\n",
            "9280 0 [1 0]\n",
            "22159 0 [0 1]\n",
            "20123 0 [0 1]\n",
            "19213 0 [0 1]\n",
            "17957 0 [0 1]\n",
            "20431 0 [0 1]\n",
            "20323 0 [0 1]\n",
            "22283 0 [0 1]\n",
            "20353 0 [0 1]\n",
            "30724 0 [1 0]\n",
            "6898 1 [1 0]\n",
            "18191 0 [0 1]\n",
            "18803 0 [0 1]\n",
            "28049 0 [1 0]\n",
            "15863 1 [1 0]\n",
            "20219 0 [0 1]\n",
            "21059 0 [0 1]\n",
            "1930 0 [1 0]\n",
            "21467 0 [0 1]\n",
            "21601 0 [0 1]\n",
            "20477 1 [0 1]\n",
            "21589 0 [0 1]\n",
            "28324 0 [1 0]\n",
            "29844 0 [1 0]\n",
            "20101 0 [0 1]\n",
            "21893 0 [0 1]\n",
            "20249 0 [0 1]\n",
            "20441 0 [0 1]\n",
            "21061 0 [0 1]\n",
            "5025 0 [1 0]\n",
            "20186 0 [1 0]\n",
            "17549 1 [1 0]\n",
            "6624 0 [1 0]\n",
            "15948 0 [1 0]\n",
            "19433 0 [0 1]\n",
            "19477 0 [0 1]\n",
            "17401 0 [0 1]\n",
            "17509 0 [0 1]\n",
            "17909 0 [0 1]\n",
            "20731 0 [0 1]\n",
            "19381 0 [0 1]\n",
            "20639 0 [0 1]\n",
            "26490 0 [1 0]\n",
            "17483 0 [0 1]\n",
            "18960 0 [1 0]\n",
            "18719 0 [0 1]\n",
            "17491 0 [0 1]\n",
            "20023 1 [0 1]\n",
            "18353 1 [0 1]\n",
            "6816 0 [1 0]\n",
            "20743 0 [0 1]\n",
            "17657 0 [0 1]\n",
            "28354 0 [1 0]\n",
            "20507 0 [0 1]\n",
            "19087 0 [0 1]\n",
            "17783 0 [0 1]\n",
            "19853 0 [0 1]\n",
            "6496 0 [1 0]\n",
            "19387 0 [0 1]\n",
            "22109 0 [0 1]\n",
            "20047 0 [0 1]\n",
            "29104 0 [1 0]\n",
            "19333 0 [0 1]\n",
            "20549 0 [0 1]\n",
            "22271 0 [0 1]\n",
            "31095 0 [1 0]\n",
            "16075 0 [1 0]\n",
            "6292 0 [1 0]\n",
            "18451 0 [0 1]\n",
            "10474 0 [1 0]\n",
            "19289 0 [0 1]\n",
            "20133 0 [1 0]\n",
            "12423 0 [1 0]\n",
            "14926 0 [1 0]\n",
            "15983 0 [1 0]\n",
            "1015 0 [1 0]\n",
            "21378 1 [1 0]\n",
            "30105 0 [1 0]\n",
            "12268 0 [1 0]\n",
            "20011 0 [0 1]\n",
            "7869 0 [1 0]\n",
            "18169 0 [0 1]\n",
            "17519 0 [0 1]\n",
            "18483 0 [1 0]\n",
            "18587 0 [0 1]\n",
            "22259 0 [0 1]\n",
            "21563 0 [0 1]\n",
            "19207 0 [0 1]\n",
            "7734 0 [1 0]\n",
            "4584 0 [1 0]\n",
            "18223 0 [0 1]\n",
            "23504 0 [1 0]\n",
            "14076 0 [1 0]\n",
            "32028 0 [1 0]\n",
            "30555 0 [1 0]\n",
            "22568 0 [1 0]\n",
            "19681 0 [0 1]\n",
            "11361 0 [1 0]\n",
            "21995 0 [1 0]\n",
            "20771 0 [0 1]\n",
            "21023 0 [0 1]\n",
            "17423 0 [1 0]\n",
            "26534 0 [1 0]\n",
            "15398 0 [1 0]\n",
            "8236 0 [1 0]\n",
            "22277 0 [0 1]\n",
            "21967 0 [1 0]\n",
            "28744 0 [1 0]\n",
            "19609 0 [0 1]\n",
            "27507 0 [1 0]\n",
            "17891 1 [0 1]\n",
            "17929 1 [0 1]\n",
            "18959 0 [0 1]\n",
            "19891 0 [0 1]\n",
            "6794 0 [1 0]\n",
            "18089 0 [0 1]\n",
            "13705 0 [1 0]\n",
            "20879 0 [0 1]\n",
            "6919 0 [1 0]\n",
            "4920 0 [1 0]\n",
            "23614 0 [1 0]\n",
            "12027 0 [1 0]\n",
            "6964 0 [1 0]\n",
            "555 0 [1 0]\n",
            "23200 0 [1 0]\n",
            "21751 1 [0 1]\n",
            "17551 0 [0 1]\n",
            "21319 0 [0 1]\n",
            "18219 0 [1 0]\n",
            "19403 0 [0 1]\n",
            "21559 0 [0 1]\n",
            "16697 0 [1 0]\n",
            "29926 0 [1 0]\n",
            "9093 0 [1 0]\n",
            "18523 1 [0 1]\n",
            "5235 0 [1 0]\n",
            "22303 0 [0 1]\n",
            "7519 0 [1 0]\n",
            "16628 0 [1 0]\n",
            "25209 0 [1 0]\n",
            "21379 0 [0 1]\n",
            "18979 1 [0 1]\n",
            "19441 0 [0 1]\n",
            "22327 0 [1 0]\n",
            "16767 0 [1 0]\n",
            "17729 0 [0 1]\n",
            "19421 0 [0 1]\n",
            "20899 0 [0 1]\n",
            "19163 0 [0 1]\n",
            "18229 0 [0 1]\n",
            "18713 0 [0 1]\n",
            "17987 0 [0 1]\n",
            "21851 0 [0 1]\n",
            "6132 0 [1 0]\n",
            "31356 0 [1 0]\n",
            "20509 0 [0 1]\n",
            "19423 0 [0 1]\n",
            "1642 0 [1 0]\n",
            "22157 0 [0 1]\n",
            "20921 0 [0 1]\n",
            "5796 0 [1 0]\n",
            "20287 0 [0 1]\n",
            "5587 1 [1 0]\n",
            "11760 0 [1 0]\n",
            "27542 0 [1 0]\n",
            "21739 0 [0 1]\n",
            "19508 0 [1 0]\n",
            "17431 1 [0 1]\n",
            "27206 1 [1 0]\n",
            "21661 0 [0 1]\n",
            "18637 0 [0 1]\n",
            "20551 0 [0 1]\n",
            "16792 0 [1 0]\n",
            "18757 0 [0 1]\n",
            "18859 0 [0 1]\n",
            "8604 0 [1 0]\n",
            "21949 0 [1 0]\n",
            "21269 0 [0 1]\n",
            "21997 0 [0 1]\n",
            "28415 1 [1 0]\n",
            "22153 0 [0 1]\n",
            "18788 1 [1 0]\n",
            "17235 0 [1 0]\n",
            "20443 0 [0 1]\n",
            "19447 0 [0 1]\n",
            "21713 0 [0 1]\n",
            "19801 0 [0 1]\n",
            "3757 1 [1 0]\n",
            "21341 0 [0 1]\n",
            "27215 1 [1 0]\n",
            "19108 0 [1 0]\n",
            "19919 0 [0 1]\n",
            "23787 0 [1 0]\n",
            "18439 0 [0 1]\n",
            "20347 0 [0 1]\n",
            "7358 0 [1 0]\n",
            "21419 0 [0 1]\n",
            "18911 0 [0 1]\n",
            "21227 0 [0 1]\n",
            "21847 0 [1 0]\n",
            "10100 0 [1 0]\n",
            "10480 0 [1 0]\n",
            "17985 0 [1 0]\n",
            "8822 1 [1 0]\n",
            "21683 0 [0 1]\n",
            "4046 1 [1 0]\n",
            "24829 0 [1 0]\n",
            "7038 0 [1 0]\n",
            "7007 0 [1 0]\n",
            "18617 0 [0 1]\n",
            "17737 0 [0 1]\n",
            "18661 0 [0 1]\n",
            "21177 0 [1 0]\n",
            "22079 0 [0 1]\n",
            "20663 1 [0 1]\n",
            "25615 0 [1 0]\n",
            "11877 1 [1 0]\n",
            "19301 0 [0 1]\n",
            "6970 0 [1 0]\n",
            "19777 1 [0 1]\n",
            "21599 0 [0 1]\n",
            "17579 1 [0 1]\n",
            "25133 0 [1 0]\n",
            "31475 0 [1 0]\n",
            "8576 0 [1 0]\n",
            "9919 1 [1 0]\n",
            "21221 0 [0 1]\n",
            "21961 0 [0 1]\n",
            "29621 0 [1 0]\n",
            "20297 1 [0 1]\n",
            "21613 0 [0 1]\n",
            "235 0 [1 0]\n",
            "21088 0 [1 0]\n",
            "30332 1 [1 0]\n",
            "27593 0 [1 0]\n",
            "18433 0 [0 1]\n",
            "7334 0 [1 0]\n",
            "18671 0 [0 1]\n",
            "21911 0 [0 1]\n",
            "9903 0 [1 0]\n",
            "20849 0 [0 1]\n",
            "21079 0 [1 0]\n",
            "1768 1 [1 0]\n",
            "11377 0 [1 0]\n",
            "18282 0 [1 0]\n",
            "21517 0 [0 1]\n",
            "26640 0 [1 0]\n",
            "17493 0 [1 0]\n",
            "18043 1 [0 1]\n",
            "22031 0 [0 1]\n",
            "12122 0 [1 0]\n",
            "27706 0 [1 0]\n",
            "17539 0 [0 1]\n",
            "21493 0 [0 1]\n",
            "8898 0 [1 0]\n",
            "21139 0 [0 1]\n",
            "13485 0 [1 0]\n",
            "17761 0 [0 1]\n",
            "22279 1 [0 1]\n",
            "20183 0 [0 1]\n",
            "4455 0 [1 0]\n",
            "22171 0 [0 1]\n",
            "22147 0 [0 1]\n",
            "9114 0 [1 0]\n",
            "17911 1 [0 1]\n",
            "6554 0 [1 0]\n",
            "18341 0 [0 1]\n",
            "15231 0 [1 0]\n",
            "22307 1 [0 1]\n",
            "16614 0 [1 0]\n",
            "20897 0 [0 1]\n",
            "13243 1 [1 0]\n",
            "21421 0 [1 0]\n",
            "19081 0 [0 1]\n",
            "21943 0 [0 1]\n",
            "19009 0 [0 1]\n",
            "20327 0 [0 1]\n",
            "21647 0 [0 1]\n",
            "26379 0 [1 0]\n",
            "17471 0 [0 1]\n",
            "3737 0 [1 0]\n",
            "18199 0 [0 1]\n",
            "19379 0 [0 1]\n",
            "30815 0 [1 0]\n",
            "20479 0 [0 1]\n",
            "1734 0 [1 0]\n",
            "10340 0 [1 0]\n",
            "20411 0 [0 1]\n",
            "2229 0 [1 0]\n",
            "2285 0 [1 0]\n",
            "20611 0 [0 1]\n",
            "3186 0 [1 0]\n",
            "17597 0 [0 1]\n",
            "22071 0 [1 0]\n",
            "22652 0 [1 0]\n",
            "22414 0 [1 0]\n",
            "18311 0 [0 1]\n",
            "18743 0 [0 1]\n",
            "18503 0 [0 1]\n",
            "2902 0 [1 0]\n",
            "17683 1 [0 1]\n",
            "28371 0 [1 0]\n",
            "1365 0 [1 0]\n",
            "24194 1 [1 0]\n",
            "19687 0 [0 1]\n",
            "17707 0 [0 1]\n",
            "20747 0 [0 1]\n",
            "18287 0 [0 1]\n",
            "20535 0 [1 0]\n",
            "23745 0 [1 0]\n",
            "5543 0 [1 0]\n",
            "11616 0 [1 0]\n",
            "20749 0 [0 1]\n",
            "19013 0 [0 1]\n",
            "6320 0 [1 0]\n",
            "17419 0 [0 1]\n",
            "26305 0 [1 0]\n",
            "11707 0 [1 0]\n",
            "8157 0 [1 0]\n",
            "864 0 [1 0]\n",
            "6566 0 [1 0]\n",
            "4035 1 [1 0]\n",
            "12590 0 [1 0]\n",
            "18077 0 [0 1]\n",
            "21737 0 [0 1]\n",
            "6657 1 [1 0]\n",
            "9582 0 [1 0]\n",
            "22091 0 [0 1]\n",
            "30213 0 [1 0]\n",
            "25029 1 [1 0]\n",
            "19157 0 [0 1]\n",
            "7527 0 [1 0]\n",
            "21193 0 [0 1]\n",
            "26697 0 [1 0]\n",
            "19469 0 [0 1]\n",
            "19139 0 [0 1]\n",
            "1566 0 [1 0]\n",
            "18493 0 [0 1]\n",
            "21377 1 [0 1]\n",
            "6941 0 [1 0]\n",
            "19141 0 [0 1]\n",
            "18461 0 [0 1]\n",
            "3466 0 [1 0]\n",
            "22093 1 [0 1]\n",
            "15041 0 [1 0]\n",
            "4745 1 [1 0]\n",
            "3588 0 [1 0]\n",
            "19073 0 [0 1]\n",
            "20947 0 [0 1]\n",
            "5338 0 [1 0]\n",
            "22291 0 [0 1]\n",
            "25428 0 [1 0]\n",
            "19913 0 [0 1]\n",
            "24360 1 [1 0]\n",
            "8050 0 [1 0]\n",
            "23678 0 [1 0]\n",
            "21211 0 [0 1]\n",
            "17713 0 [0 1]\n",
            "15496 0 [1 0]\n",
            "21401 0 [0 1]\n",
            "21767 0 [0 1]\n",
            "20333 0 [0 1]\n",
            "20959 0 [0 1]\n",
            "19597 0 [0 1]\n",
            "17477 1 [0 1]\n",
            "19937 0 [0 1]\n",
            "22273 0 [0 1]\n",
            "20753 0 [0 1]\n",
            "21407 0 [0 1]\n",
            "17749 0 [0 1]\n",
            "26725 0 [1 0]\n",
            "9575 0 [1 0]\n",
            "20147 0 [0 1]\n",
            "22067 1 [0 1]\n",
            "20759 0 [0 1]\n",
            "19457 0 [0 1]\n",
            "19867 1 [0 1]\n",
            "18731 0 [0 1]\n",
            "22133 0 [0 1]\n",
            "13900 1 [1 0]\n",
            "19560 0 [1 0]\n",
            "20393 0 [0 1]\n",
            "17851 1 [0 1]\n",
            "22852 0 [1 0]\n",
            "21481 0 [0 1]\n",
            "13839 1 [1 0]\n",
            "18413 0 [0 1]\n",
            "20063 1 [0 1]\n",
            "21734 0 [1 0]\n",
            "22063 0 [0 1]\n",
            "13456 0 [1 0]\n",
            "21518 0 [1 0]\n",
            "20543 0 [0 1]\n",
            "25635 0 [1 0]\n",
            "30644 0 [1 0]\n",
            "32139 0 [1 0]\n",
            "20887 0 [0 1]\n",
            "19501 0 [0 1]\n",
            "17443 0 [0 1]\n",
            "13039 0 [1 0]\n",
            "25646 0 [1 0]\n",
            "22013 0 [0 1]\n",
            "23595 0 [1 0]\n",
            "19813 0 [0 1]\n",
            "19531 0 [0 1]\n",
            "29861 0 [1 0]\n",
            "14822 0 [1 0]\n",
            "18379 0 [0 1]\n",
            "25684 0 [1 0]\n",
            "11905 0 [1 0]\n",
            "24842 0 [1 0]\n",
            "28234 0 [1 0]\n",
            "19427 0 [0 1]\n",
            "18797 0 [0 1]\n",
            "5412 1 [1 0]\n",
            "31596 1 [1 0]\n",
            "29475 0 [1 0]\n",
            "21121 0 [0 1]\n",
            "11048 0 [1 0]\n",
            "22357 0 [1 0]\n",
            "18691 1 [0 1]\n",
            "22229 0 [0 1]\n",
            "21773 0 [0 1]\n",
            "20269 0 [0 1]\n",
            "21190 0 [1 0]\n",
            "639 1 [1 0]\n",
            "32650 0 [1 0]\n",
            "19211 0 [0 1]\n",
            "2989 0 [1 0]\n",
            "21011 0 [0 1]\n",
            "17417 0 [0 1]\n",
            "21937 0 [0 1]\n",
            "18401 0 [0 1]\n",
            "20929 0 [0 1]\n",
            "16852 0 [1 0]\n",
            "21397 0 [0 1]\n",
            "4144 0 [1 0]\n",
            "21803 0 [0 1]\n",
            "18121 0 [0 1]\n",
            "18457 0 [0 1]\n",
            "7824 0 [1 0]\n",
            "7542 0 [1 0]\n",
            "32005 0 [1 0]\n",
            "17789 1 [0 1]\n",
            "21841 0 [0 1]\n",
            "22445 0 [1 0]\n",
            "6539 0 [1 0]\n",
            "21787 0 [0 1]\n",
            "13929 0 [1 0]\n",
            "32739 0 [1 0]\n",
            "1159 0 [1 0]\n",
            "14790 0 [1 0]\n",
            "19963 0 [0 1]\n",
            "19219 0 [0 1]\n",
            "21812 0 [1 0]\n",
            "21799 0 [0 1]\n",
            "3309 0 [1 0]\n",
            "18443 0 [0 1]\n",
            "18807 0 [1 0]\n",
            "29897 0 [1 0]\n",
            "19991 0 [0 1]\n",
            "22515 0 [1 0]\n",
            "6017 0 [1 0]\n",
            "6772 1 [1 0]\n",
            "17959 0 [0 1]\n",
            "18059 0 [0 1]\n",
            "23645 0 [1 0]\n",
            "1415 0 [1 0]\n",
            "26957 0 [1 0]\n",
            "12684 1 [1 0]\n",
            "21247 0 [0 1]\n",
            "18534 0 [1 0]\n",
            "7066 0 [1 0]\n",
            "17659 0 [0 1]\n",
            "19661 0 [0 1]\n",
            "21283 0 [0 1]\n",
            "32630 1 [1 0]\n",
            "28431 0 [1 0]\n",
            "17837 0 [0 1]\n",
            "18749 1 [0 1]\n",
            "12813 0 [1 0]\n",
            "21179 0 [0 1]\n",
            "6896 0 [1 0]\n",
            "23814 1 [1 0]\n",
            "18899 0 [0 1]\n",
            "605 0 [1 0]\n",
            "21575 0 [1 0]\n",
            "19079 1 [0 1]\n",
            "24266 0 [1 0]\n",
            "18307 0 [0 1]\n",
            "19819 0 [0 1]\n",
            "2379 0 [1 0]\n",
            "22111 0 [0 1]\n",
            "22840 1 [1 0]\n",
            "17627 0 [0 1]\n",
            "18593 1 [0 1]\n",
            "15676 0 [1 0]\n",
            "25370 0 [1 0]\n",
            "19373 0 [0 1]\n",
            "24375 0 [1 0]\n",
            "17903 0 [0 1]\n",
            "27139 0 [1 0]\n",
            "12159 0 [1 0]\n",
            "17921 0 [0 1]\n",
            "1791 0 [1 0]\n",
            "21611 0 [0 1]\n",
            "31478 0 [1 0]\n",
            "17569 0 [0 1]\n",
            "21544 0 [1 0]\n",
            "8299 0 [1 0]\n",
            "20389 0 [0 1]\n",
            "24973 1 [1 0]\n",
            "22051 0 [0 1]\n",
            "21391 0 [0 1]\n",
            "10800 0 [1 0]\n",
            "30498 0 [1 0]\n",
            "19181 0 [0 1]\n",
            "18539 0 [0 1]\n",
            "23309 0 [1 0]\n",
            "23148 0 [1 0]\n",
            "20201 0 [0 1]\n",
            "15068 0 [1 0]\n",
            "19092 0 [1 0]\n",
            "5331 0 [1 0]\n",
            "18181 0 [0 1]\n",
            "21089 0 [0 1]\n",
            "21169 1 [0 1]\n",
            "20071 0 [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAD1Golvo17G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor,MLPClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZzibtOJpXRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MLPRegressor(hidden_layer_sizes=(128,128,128,128,128,128,128,128,128,128),n_iter_no_change=200,random_state=581,learning_rate='adaptive',verbose=True,max_iter=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6slNyXDApihj",
        "colab_type": "code",
        "outputId": "47a04d3e-5789-48ea-8050-7125139070e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train,y_train.ravel())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 18459245784.24534225\n",
            "Iteration 2, loss = 916011887.72037065\n",
            "Iteration 3, loss = 940682927.91360784\n",
            "Iteration 4, loss = 1007231605.15223050\n",
            "Iteration 5, loss = 1003319957.78973591\n",
            "Iteration 6, loss = 1001481350.74837565\n",
            "Iteration 7, loss = 997114138.88577974\n",
            "Iteration 8, loss = 995729567.36030579\n",
            "Iteration 9, loss = 961379958.51730621\n",
            "Iteration 10, loss = 973054358.57994246\n",
            "Iteration 11, loss = 971437989.32615185\n",
            "Iteration 12, loss = 935947036.70375359\n",
            "Iteration 13, loss = 8107750323.40449715\n",
            "Iteration 14, loss = 838426049.64324963\n",
            "Iteration 15, loss = 848994929.07969081\n",
            "Iteration 16, loss = 877813252.47678280\n",
            "Iteration 17, loss = 882051950.93676364\n",
            "Iteration 18, loss = 892339717.94924581\n",
            "Iteration 19, loss = 892902867.68447661\n",
            "Iteration 20, loss = 898025894.04637706\n",
            "Iteration 21, loss = 866374350.39391530\n",
            "Iteration 22, loss = 869754768.44160950\n",
            "Iteration 23, loss = 824261825.12934172\n",
            "Iteration 24, loss = 820113188.32413065\n",
            "Iteration 25, loss = 785451017.87329674\n",
            "Iteration 26, loss = 673834017.67674065\n",
            "Iteration 27, loss = 573855908.06890011\n",
            "Iteration 28, loss = 490906098.26661193\n",
            "Iteration 29, loss = 407576556.36819881\n",
            "Iteration 30, loss = 303807032.39205343\n",
            "Iteration 31, loss = 259958836.07365918\n",
            "Iteration 32, loss = 266559764.56276187\n",
            "Iteration 33, loss = 428521667.62212491\n",
            "Iteration 34, loss = 319670630.53454512\n",
            "Iteration 35, loss = 282438290.24138236\n",
            "Iteration 36, loss = 303026293.52742732\n",
            "Iteration 37, loss = 183843108.88523367\n",
            "Iteration 38, loss = 116262117.32162228\n",
            "Iteration 39, loss = 103902613.68693949\n",
            "Iteration 40, loss = 57466916.82837371\n",
            "Iteration 41, loss = 46329069.17123975\n",
            "Iteration 42, loss = 158396108.08817226\n",
            "Iteration 43, loss = 72431659.39490938\n",
            "Iteration 44, loss = 30218217.15829486\n",
            "Iteration 45, loss = 70672378.99150689\n",
            "Iteration 46, loss = 26519024.55608398\n",
            "Iteration 47, loss = 31459272.11013725\n",
            "Iteration 48, loss = 41520009.94132198\n",
            "Iteration 49, loss = 27176840.66154693\n",
            "Iteration 50, loss = 124828623.30380046\n",
            "Iteration 51, loss = 14884923.90632834\n",
            "Iteration 52, loss = 62330970.22520500\n",
            "Iteration 53, loss = 28055286.69044767\n",
            "Iteration 54, loss = 34782288.33825649\n",
            "Iteration 55, loss = 24047744.04918588\n",
            "Iteration 56, loss = 38812025.68996486\n",
            "Iteration 57, loss = 14870475.91716847\n",
            "Iteration 58, loss = 31033970.90456725\n",
            "Iteration 59, loss = 34758265.57175852\n",
            "Iteration 60, loss = 14343027.67715588\n",
            "Iteration 61, loss = 241526127.05308253\n",
            "Iteration 62, loss = 14206288.10274971\n",
            "Iteration 63, loss = 21809126.10077757\n",
            "Iteration 64, loss = 65591453.76258913\n",
            "Iteration 65, loss = 13239442.86413763\n",
            "Iteration 66, loss = 35099461.24677550\n",
            "Iteration 67, loss = 22083969.29400770\n",
            "Iteration 68, loss = 49617892.87375054\n",
            "Iteration 69, loss = 10368533.82214300\n",
            "Iteration 70, loss = 24368819.81870412\n",
            "Iteration 71, loss = 28104400.29786092\n",
            "Iteration 72, loss = 17491919.19043378\n",
            "Iteration 73, loss = 41419313.98901130\n",
            "Iteration 74, loss = 16211524.28484647\n",
            "Iteration 75, loss = 21082541.81018854\n",
            "Iteration 76, loss = 65369825.47051436\n",
            "Iteration 77, loss = 16968741.77534092\n",
            "Iteration 78, loss = 21807497.41468078\n",
            "Iteration 79, loss = 22837026.29558146\n",
            "Iteration 80, loss = 18744570.02015480\n",
            "Iteration 81, loss = 24182152.32193401\n",
            "Iteration 82, loss = 29846895.72971932\n",
            "Iteration 83, loss = 17044188.81855582\n",
            "Iteration 84, loss = 19133060.80124830\n",
            "Iteration 85, loss = 58542316.17877409\n",
            "Iteration 86, loss = 1495724.91740402\n",
            "Iteration 87, loss = 21881092.92564987\n",
            "Iteration 88, loss = 22241241.35895189\n",
            "Iteration 89, loss = 15523821.89854358\n",
            "Iteration 90, loss = 26620974.01662635\n",
            "Iteration 91, loss = 17858738.95593881\n",
            "Iteration 92, loss = 18873496.36228231\n",
            "Iteration 93, loss = 17574305.99599450\n",
            "Iteration 94, loss = 28647175.27941032\n",
            "Iteration 95, loss = 10891826.65328906\n",
            "Iteration 96, loss = 24320086.29915961\n",
            "Iteration 97, loss = 56080230.50600141\n",
            "Iteration 98, loss = 2505241.99435191\n",
            "Iteration 99, loss = 19281885.67216318\n",
            "Iteration 100, loss = 24258267.33415354\n",
            "Iteration 101, loss = 13591944.33815302\n",
            "Iteration 102, loss = 22309083.95986058\n",
            "Iteration 103, loss = 14009596.15551763\n",
            "Iteration 104, loss = 36666091.51029499\n",
            "Iteration 105, loss = 7630751.46854622\n",
            "Iteration 106, loss = 19716401.34956604\n",
            "Iteration 107, loss = 14139001.01715153\n",
            "Iteration 108, loss = 16161805.62359028\n",
            "Iteration 109, loss = 36414449.82401559\n",
            "Iteration 110, loss = 7943275.81104260\n",
            "Iteration 111, loss = 17286466.72892390\n",
            "Iteration 112, loss = 15681684.19447659\n",
            "Iteration 113, loss = 12801402.48030373\n",
            "Iteration 114, loss = 15240961.75027241\n",
            "Iteration 115, loss = 23290654.11011423\n",
            "Iteration 116, loss = 12490784.09393027\n",
            "Iteration 117, loss = 15208809.61181890\n",
            "Iteration 118, loss = 14630490.45829071\n",
            "Iteration 119, loss = 24653868.70061410\n",
            "Iteration 120, loss = 8378445.94993706\n",
            "Iteration 121, loss = 27777752.92738291\n",
            "Iteration 122, loss = 5397410.24936772\n",
            "Iteration 123, loss = 14518235.22885565\n",
            "Iteration 124, loss = 16204481.13177997\n",
            "Iteration 125, loss = 15125165.94938938\n",
            "Iteration 126, loss = 12608595.11875387\n",
            "Iteration 127, loss = 15084873.92219305\n",
            "Iteration 128, loss = 15359458.60094697\n",
            "Iteration 129, loss = 21309761.99068353\n",
            "Iteration 130, loss = 9728487.09909781\n",
            "Iteration 131, loss = 11183523.89716615\n",
            "Iteration 132, loss = 13407261.25088063\n",
            "Iteration 133, loss = 17663521.71462872\n",
            "Iteration 134, loss = 10316009.65304355\n",
            "Iteration 135, loss = 13606604.63252491\n",
            "Iteration 136, loss = 10748861.70402155\n",
            "Iteration 137, loss = 15069848.22367407\n",
            "Iteration 138, loss = 11097318.90085608\n",
            "Iteration 139, loss = 11236361.57936696\n",
            "Iteration 140, loss = 14343454.52709863\n",
            "Iteration 141, loss = 10078486.05001741\n",
            "Iteration 142, loss = 13645019.08179662\n",
            "Iteration 143, loss = 16039718.99655790\n",
            "Iteration 144, loss = 9557495.60503024\n",
            "Iteration 145, loss = 10399171.59507569\n",
            "Iteration 146, loss = 12454586.93764311\n",
            "Iteration 147, loss = 12058099.06915450\n",
            "Iteration 148, loss = 11826977.84039525\n",
            "Iteration 149, loss = 10735383.98568616\n",
            "Iteration 150, loss = 10969935.68953386\n",
            "Iteration 151, loss = 12531477.92166420\n",
            "Iteration 152, loss = 14102092.23828794\n",
            "Iteration 153, loss = 8266300.03043615\n",
            "Iteration 154, loss = 13309695.33321068\n",
            "Iteration 155, loss = 11495416.29214404\n",
            "Iteration 156, loss = 9974639.72968476\n",
            "Iteration 157, loss = 13225899.83731399\n",
            "Iteration 158, loss = 10956263.81338712\n",
            "Iteration 159, loss = 8558276.66411829\n",
            "Iteration 160, loss = 13379467.96951361\n",
            "Iteration 161, loss = 14365937.29916526\n",
            "Iteration 162, loss = 11526990.50781082\n",
            "Iteration 163, loss = 9588735.11966657\n",
            "Iteration 164, loss = 11278928.03192475\n",
            "Iteration 165, loss = 13563012.89536142\n",
            "Iteration 166, loss = 19352789.06603630\n",
            "Iteration 167, loss = 4528656.28125831\n",
            "Iteration 168, loss = 11808461.89770797\n",
            "Iteration 169, loss = 11204155.43855689\n",
            "Iteration 170, loss = 10658953.50208290\n",
            "Iteration 171, loss = 11697453.24583763\n",
            "Iteration 172, loss = 10537294.16909889\n",
            "Iteration 173, loss = 11589384.63270628\n",
            "Iteration 174, loss = 10191603.85226031\n",
            "Iteration 175, loss = 9728913.44570585\n",
            "Iteration 176, loss = 8795600.45318404\n",
            "Iteration 177, loss = 11542202.13391415\n",
            "Iteration 178, loss = 14125733.99594046\n",
            "Iteration 179, loss = 8159800.57582594\n",
            "Iteration 180, loss = 11564214.64948497\n",
            "Iteration 181, loss = 9375858.59445308\n",
            "Iteration 182, loss = 10423941.50187652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "             hidden_layer_sizes=(128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
              "                                 128),\n",
              "             learning_rate='adaptive', learning_rate_init=0.001, max_fun=15000,\n",
              "             max_iter=200, momentum=0.9, n_iter_no_change=200,\n",
              "             nesterovs_momentum=True, power_t=0.5, random_state=581,\n",
              "             shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
              "             verbose=True, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PCF-7tYpu_S",
        "colab_type": "code",
        "outputId": "a10db8c9-cc66-472f-d7d0-26e5eabc0052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.score(X_test,y_test)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9994007416728249"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VchPuxx3qY8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69390e5f-0be6-44b5-94af-3169ef9404ea"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvuGHPIKEnmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "623adf6f-7d9c-4ef4-e9e2-e3278bcf874c"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[      2],\n",
              "       [      3],\n",
              "       [      5],\n",
              "       ...,\n",
              "       [2750123],\n",
              "       [2750131],\n",
              "       [2750159]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8aJyHxEpAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "de4a68d6-2162-440b-f7a1-6437c0405e0a"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[     1],\n",
              "       [     2],\n",
              "       [     3],\n",
              "       ...,\n",
              "       [199998],\n",
              "       [199999],\n",
              "       [200000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSelW1gSIYr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "64e48486-0fc3-4dee-a91f-23e09fd14bd7"
      },
      "source": [
        "model.predict(X_train)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.19652337e+00, 5.31014987e+00, 6.06385092e+00, ...,\n",
              "       2.74976992e+06, 2.74978456e+06, 2.74979921e+06])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZJgyitWJpjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}