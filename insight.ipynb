{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.utils import np_utils\n",
    "from math import ceil,floor\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data consists of 9000 numbers in binary representation(n bits) and test contains 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig=np.load('xtrain.npy').reshape(-1,n_bits)\n",
    "y_train_orig=np.load('ytrain.npy').reshape(-1,1)\n",
    "X_test=np.load('xtest.npy').reshape(-1,n_bits)\n",
    "y_test=np.load('ytest.npy').reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orig.shape\n",
    "y_train_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_size=10000\n",
    "X_train=X_train_orig[:training_data_size]\n",
    "y_train=y_train_orig[:training_data_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 9000 epochs 200 acc 0.9990000 inc 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the simplest possible model i.e single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 257\n",
      "Trainable params: 257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1,activation='relu',input_shape=(n_bits,)))\n",
    "adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=adam,metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model has only n_bits+1 parameters, 1 weight for each bit and a bias value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using early stopping and learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 3.6123 - accuracy: 0.5106\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 2.8850 - accuracy: 0.5410\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 2.7895 - accuracy: 0.5866\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 2.2800 - accuracy: 0.6056\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 4.1179 - accuracy: 0.3695\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 1.9224 - accuracy: 0.6157\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 1.6750 - accuracy: 0.6564\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.7317 - accuracy: 0.6291\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.3234 - accuracy: 0.6754\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1473 - accuracy: 0.6969\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0109 - accuracy: 0.7140\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.5268 - accuracy: 0.7155\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.7215 - accuracy: 0.7496\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0632 - accuracy: 0.7583\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 2.5502 - accuracy: 0.6634\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 5.7920 - accuracy: 0.5830\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 8.1573 - accuracy: 0.4930\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 8.1534 - accuracy: 0.4930\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 8.1453 - accuracy: 0.4932\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 8.1341 - accuracy: 0.4934\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 8.0594 - accuracy: 0.4937\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 5.7607 - accuracy: 0.5585\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.8901 - accuracy: 0.7285\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.8442 - accuracy: 0.7316\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.7372 - accuracy: 0.7551\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.7165 - accuracy: 0.7500\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.7115 - accuracy: 0.7558\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.7195 - accuracy: 0.7624\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.7185 - accuracy: 0.7659\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.6937 - accuracy: 0.7588\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.6858 - accuracy: 0.7537\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.6662 - accuracy: 0.7585\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.6617 - accuracy: 0.7558\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.6464 - accuracy: 0.7618\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.6344 - accuracy: 0.7610\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.6328 - accuracy: 0.7668\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.6082 - accuracy: 0.7615\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.5908 - accuracy: 0.7780\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.6116 - accuracy: 0.7868\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.5741 - accuracy: 0.7850\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.5468 - accuracy: 0.7880\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.5433 - accuracy: 0.7892\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.5223 - accuracy: 0.7873\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.5003 - accuracy: 0.7891\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 0.4841 - accuracy: 0.7918\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.4697 - accuracy: 0.7923\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4556 - accuracy: 0.7959\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4580 - accuracy: 0.7831\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.4282 - accuracy: 0.8119\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 32us/step - loss: 0.4105 - accuracy: 0.8071\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.4228 - accuracy: 0.8166\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.4154 - accuracy: 0.7588\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.3740 - accuracy: 0.8009\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.3624 - accuracy: 0.8161\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.3621 - accuracy: 0.8213\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.3581 - accuracy: 0.8255\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.3527 - accuracy: 0.8226\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.3452 - accuracy: 0.8319\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.3385 - accuracy: 0.8340\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.3253 - accuracy: 0.8308\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.3142 - accuracy: 0.8265\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.3162 - accuracy: 0.8028\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.2983 - accuracy: 0.8098\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.2799 - accuracy: 0.8322\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.2793 - accuracy: 0.8489\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.2575 - accuracy: 0.8428\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.2465 - accuracy: 0.8395\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.2313 - accuracy: 0.8470\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.2225 - accuracy: 0.8518\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.2137 - accuracy: 0.8537\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.2047 - accuracy: 0.8609\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.2049 - accuracy: 0.8688\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.1860 - accuracy: 0.8744\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.1754 - accuracy: 0.8803\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1665 - accuracy: 0.8727\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.1580 - accuracy: 0.8801\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.1512 - accuracy: 0.8815\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.1418 - accuracy: 0.8887\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1322 - accuracy: 0.9035\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.1202 - accuracy: 0.9026\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.1120 - accuracy: 0.9035\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.1018 - accuracy: 0.9122\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.0913 - accuracy: 0.9148\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 0.0831 - accuracy: 0.9229\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0749 - accuracy: 0.9276\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0665 - accuracy: 0.9323\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0584 - accuracy: 0.9381\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.0491 - accuracy: 0.9439\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 0.0412 - accuracy: 0.9482\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 18us/step - loss: 0.0339 - accuracy: 0.9530\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0273 - accuracy: 0.9577\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 0.0213 - accuracy: 0.9607\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.0162 - accuracy: 0.9660\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 19us/step - loss: 0.0119 - accuracy: 0.9682\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 0.0086 - accuracy: 0.9736\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.0060 - accuracy: 0.9775\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 0.0042 - accuracy: 0.9804\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 0.0029 - accuracy: 0.9834\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 0.0020 - accuracy: 0.9849\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0014 - accuracy: 0.9878\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 9.8346e-04 - accuracy: 0.9884\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 6.9504e-04 - accuracy: 0.9900\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 4.8451e-04 - accuracy: 0.9909\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 22us/step - loss: 3.4249e-04 - accuracy: 0.9913\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 21us/step - loss: 2.4725e-04 - accuracy: 0.9924\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.6690e-04 - accuracy: 0.9928\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.1562e-04 - accuracy: 0.9941\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 7.7408e-05 - accuracy: 0.9949\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 4.9304e-05 - accuracy: 0.9948\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 2.9502e-05 - accuracy: 0.9954\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.6658e-05 - accuracy: 0.9958\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1.2882e-05 - accuracy: 0.9953\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 6.0173e-06 - accuracy: 0.9953\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.2644e-06 - accuracy: 0.9965\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 1s 50us/step - loss: 5.6082e-06 - accuracy: 0.9956\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 3.7318e-06 - accuracy: 0.9964\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 1.4785e-06 - accuracy: 0.9961\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 20us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 24us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 23us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0974e-07 - accuracy: 0.9964\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 0s 25us/step - loss: 1.0974e-07 - accuracy: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f46c446e940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop=keras.callbacks.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=40, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "reduce_lr=keras.callbacks.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "model.fit(X_train,y_train,epochs=200,callbacks=[reduce_lr,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 0s 41us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.996399998664856"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for Training and evaluating the model for multiple training data sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(train_size):\n",
    "    X_train=X_train_orig[:train_size]\n",
    "    y_train=y_train_orig[:train_size]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1,activation='relu',input_shape=(n_bits,)))\n",
    "    adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=adam,metrics = ['accuracy'])\n",
    "    early_stop=keras.callbacks.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=40, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    reduce_lr=keras.callbacks.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    model.fit(X_train,y_train,epochs=200,callbacks=[reduce_lr,early_stop],verbose=0)\n",
    "    return model.evaluate(X_test,y_test,verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=[]\n",
    "accuracies=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model for sizes from 100 to 10000 in increments of 200\n",
    "Corresponding to each dataset size the model is trained and evaluated 5 times and best performance is taken as it was found that model performance was heavily dependent on on initial random initialization and sometimes got stuck in local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.5192000269889832\n",
      "300 0.48420000076293945\n",
      "500 0.47760000824928284\n",
      "700 0.6592000126838684\n",
      "900 0.5651999711990356\n",
      "1100 0.8722000122070312\n",
      "1300 0.8677999973297119\n",
      "1500 0.8655999898910522\n",
      "1700 0.9197999835014343\n",
      "1900 0.9945999979972839\n",
      "2100 0.8848000168800354\n",
      "2300 0.9927999973297119\n",
      "2500 0.9797999858856201\n",
      "2700 0.9940000176429749\n",
      "2900 0.9995999932289124\n",
      "3100 0.9778000116348267\n",
      "3300 0.9733999967575073\n",
      "3500 0.9986000061035156\n",
      "3700 1.0\n",
      "3900 0.996999979019165\n",
      "4100 0.9987999796867371\n",
      "4300 0.9272000193595886\n",
      "4500 0.9994000196456909\n",
      "4700 0.9998000264167786\n",
      "4900 0.9998000264167786\n",
      "5100 0.991599977016449\n",
      "5300 0.9976000189781189\n",
      "5500 0.9941999912261963\n",
      "5700 1.0\n",
      "5900 0.9998000264167786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-398d5523f5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmaxacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mmaxacc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxacc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxacc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-eb87b842c347>\u001b[0m in \u001b[0;36mevalModel\u001b[0;34m(train_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mreduce_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcooldown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3035\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3037\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3038\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100,10000,200):\n",
    "    maxacc=0\n",
    "    for j in range(5):\n",
    "        maxacc=max(evalModel(i),maxacc)\n",
    "    train_sizes.append(i)\n",
    "    accuracies.append(maxacc)\n",
    "    print(i,maxacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_np=np.array(train_sizes)\n",
    "accuracies_np=np.array(accuracies)\n",
    "#np.save('train_sizes',train_sizes_np)\n",
    "#np.save('accuracies',accuracies_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of training data size(x axis) vs accuracy(y axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=np.load('train_sizes.npy')\n",
    "accuracies=np.load('accuracies.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Test Accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_sizes,accuracies)\n",
    "plt.xlabel(\"Training data size\")\n",
    "plt.ylabel(\"Test Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2506\n",
      "           1       1.00      0.99      0.99      2494\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      5000\n",
      "   macro avg       0.99      0.99      0.99      5000\n",
      "weighted avg       0.99      0.99      0.99      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "for i in range(pred.shape[0]):\n",
    "    pred[i]=floor(pred[i])\n",
    "\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding incorrectly predicted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "indices = [i for i,v in enumerate(pred) if pred[i]!=y_test[i]]\n",
    "subset_of_wrongly_predicted = [X_test[i] for i in indices ]\n",
    "len(subset_of_wrongly_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0111001000100110110100111110111010010000011110000001010000101101010010101101000010101111000111000111010110010100001010100111001010110010101111110101000001110100111111011111000001101001010010100101101001000001000001000011010011011010100100000000110001101100\n",
      "0010101100010000111010000001100001010011101010101100100101000100001100001011101111100110100000111011111010100011110010111110100001011010000001000100101000100010000101110100010000011001000001111111011010000110101110001010010010000000111001011000011110111000\n",
      "1001100100100010110010001100111110001000011111001010110100001010111110011001000111011010101110100111011011010100110001000101110101100010101100001010000001100100110111000000100001100111111100110011100000101100000000110001000110001000001100111001110001111010\n",
      "1111000101111000111000101010111100101000011000101111001001100100011010000011011100101001011100000100010101001000001110000111100010000111000011001110011010110011010100011000001100000111111010001000000101101000110100000011000001011000000011111000100001100010\n",
      "1010011010110101010000000101010101100001000110010001000001010010000110001010000100001101100011000001000010100001000001010010010010101001101101000110000000101100100110001111010011101101101100111110110001101001010100011010100010001110111111100010011110101110\n",
      "1001101000010100001000110010001000001110001101101000111000000101011010011010010100000100000100001001010110110001011001110101111001110110111011001010111010001011001110110010010000001000111110000000100100101010011000111110111000100000111010001100110010000000\n",
      "0011111011101011001001110101001111011100000101101001111000001010011110001101001001011100000010000001000010101110011100101001001110111001101001001110110010001111001110100000100111000100101010000000000000000000010010011011100101000110100111101000011100110010\n",
      "0010111100001010011010000011000000001111110010001100010010101100101000010110100001001101101110001101010010000001000011100010000001101100101101111110011000010101001110101000110001100010010001111001101010011100111010001000000100010110110010111001000111001000\n",
      "1000000000000100001010010010100101100000000011111100110001111011111000100110111100110001000101001011011101110010010010011011001000111001001010010001101000101011001110100101000101100011101100100011000100011100111000000100011101100010010101001000001100011110\n",
      "0111000110110001110000001010110100000110001010100000010010000010010100011111001100100101000010111101010011100001100110100110110001000000000000101100100110011101011000000001000000100010011001011010001000101001001011001011101100010011010000010100000111010100\n",
      "1011010001111001001110100101000000101101111101011101110001111011010000000010010000000100000110001011110000010111010000000101001001101000101111010010111011101110000100001000010101010000010000100001110011001000011011001011010111100001011111111000111010101000\n",
      "0101000010001000110001000101100000001110110010101101001101010111111110000110001011101001001011001000000001000111100010001001110100000110001110101001010011010011101011000010110001010000011100110101000010000111010011000101101101010101001001100110000111000000\n",
      "0000001111000011001001011001001011010000110110101001000000000001110101111000110111101001100001111011011110111010000100010100001011110100000100000101001010110111111100100001011000011000000110111111101000000000011011001101110010011100000110100000001010010100\n",
      "1000000010101010100100011011100101001110010011101111011000011011001100010111010011101100011101011001011111100110000010000011100001101110011100100011100000111001111101010101000101000110101110111100010001101010000100100011000101000001000001000110100011010110\n",
      "1111000110110010010000100100110111001111001011011010000100001111110000111010100011010000110001100001100011101111101100001110010001000101001111001010001000101111011101101111101000011011010000011111000000100000100111000001111001000010011100101100010000000000\n",
      "0000011000000101000101101001001100010111110110010100010111100011110100110100100000011110110001111001110110101100100000010100000000000011000000000000000100011011011000011001100000001000111111001011001000011111011011110010100110011111101110001001100001011110\n",
      "1010110101101010100111100101100010100101101010001100110011101010011101011110100111101000100100101000100100011000000100111010011001011011001010101111100101000011100010100110100011100011010000101000100000001000010001010001110001001010010011101110010101001010\n",
      "0110010011010010110000100100010110000100100010010101000010001000110101111100001001001001111010011001011101100000011010011100000010100011110100000000011111001000011100110010111110101010000001000011101110000110111111110010001001010101010001101100010110101010\n",
      "0000000110011111100101001010110000001011010010101101010000100111100011000110101100011001000000100000010001000100001100111011110011000001011100010000101011111011000110111000110111011011010001100011000001001010001000101010101111110110111000101000110000001110\n",
      "0001010000001110001111111111000101000001000100001110000001001000000110111111110010011001010001110001100010001101010111011000111111010010101111111100001001110011101101001000000010010000100000100100011000110010000000010001010111010101000001110100000110110100\n",
      "0110010100000011000010000101110000101101010100011011001000101000000000111111110010110000100110000000110101110000000001011100000100000100010101000100110010011110100111111000000010100100111010000100110011011010110101110000011101011011101101111010111100110100\n",
      "1011100101110010000110000001010100110110001000101111100010001000100110010011001111010101010010100010101000111100001010100101010010010100000000100001010111101001001110011000111000111001011001001111000010010001100111000001110011010010001101000111101001101000\n",
      "0001011100110010100000010101100001011100010011101111101100111010010100101111100110001001100010010001000110100010000111001110100100011100000111010010101001100110101101101010011101110001000101001111000010111110011100111000001101101011110100101010000001100010\n",
      "0101000001000100110010111001100111011000000100111010110100000110001110000010111101011000101000001101010010001010100001111001110001010010100000001000100000111100001101100110010001101001100010001100001111010000011011010111000010000010101100000010111011110100\n",
      "0001001110010011000000100010010001010111111000100100101111011111110010001001100010001000000001010001101000010000110000100101010001111011111001100010001000101000000010100010110011101011101011001111000011001000011110010110000001011100100110001000010101110010\n",
      "0100011110100111001100101100111000001000010011100110000011010011101000100110110000011100011110001000011010101001110010110111011001011011010111001000011001000010001011100001001001110011000011101110011001010110000101101000110100001111001001001000010010011010\n",
      "0100100111111010011010011011101010100010010001001000100000100110111000100100000100111010011111111101000001111100110011010101010000001010011010111101001001000101011000100000101011000011100000010010000000000011111111110000001000011011100101001100110111000010\n",
      "0000111111101111100001100100110111000010001011000100000000001000001011010010100010010100011110101110010000111001001010010101101111001001000000100011010010100100110010010000001000100001001101111010101011001111000011001001011111101011000110001011110000000100\n"
     ]
    }
   ],
   "source": [
    "for i in subset_of_wrongly_predicted:\n",
    "    \n",
    "    print(''.join(map(str, i.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 9.48245451e-03],\n",
      "       [ 1.25293005e-02],\n",
      "       [ 1.09989550e-02],\n",
      "       [ 9.24004987e-03],\n",
      "       [ 1.28578460e-02],\n",
      "       [ 8.85494519e-03],\n",
      "       [ 1.66592617e-02],\n",
      "       [-3.66466906e-04],\n",
      "       [ 1.28666526e-02],\n",
      "       [ 1.95382945e-02],\n",
      "       [ 1.37845119e-02],\n",
      "       [ 2.99780536e-03],\n",
      "       [ 6.05447544e-03],\n",
      "       [ 8.68010987e-03],\n",
      "       [-2.93785986e-03],\n",
      "       [ 2.84046028e-02],\n",
      "       [ 5.41815674e-03],\n",
      "       [ 1.11848405e-02],\n",
      "       [ 1.13366097e-02],\n",
      "       [ 1.06764073e-02],\n",
      "       [ 1.41940173e-03],\n",
      "       [ 8.30758177e-03],\n",
      "       [ 6.92257937e-03],\n",
      "       [ 1.33427335e-02],\n",
      "       [ 5.12324926e-03],\n",
      "       [ 6.26291265e-04],\n",
      "       [ 1.06272073e-02],\n",
      "       [ 2.52951891e-03],\n",
      "       [ 1.23886596e-02],\n",
      "       [ 8.93076044e-03],\n",
      "       [ 7.01619778e-03],\n",
      "       [ 1.72126223e-03],\n",
      "       [ 8.91897269e-03],\n",
      "       [ 2.09326763e-02],\n",
      "       [ 1.85775179e-02],\n",
      "       [ 3.65612209e-02],\n",
      "       [ 9.64839198e-03],\n",
      "       [-1.16299568e-02],\n",
      "       [ 9.16241575e-03],\n",
      "       [ 1.15158055e-02],\n",
      "       [ 1.30108818e-02],\n",
      "       [-7.09624076e-03],\n",
      "       [ 8.35361797e-03],\n",
      "       [ 3.69054731e-03],\n",
      "       [-1.81127377e-02],\n",
      "       [ 6.85720285e-03],\n",
      "       [ 1.53815448e-02],\n",
      "       [ 2.97022592e-02],\n",
      "       [-2.73277890e-03],\n",
      "       [ 3.67521588e-03],\n",
      "       [-8.23252994e-05],\n",
      "       [ 7.88153149e-04],\n",
      "       [ 9.91000049e-03],\n",
      "       [ 1.39450552e-02],\n",
      "       [ 2.09633019e-02],\n",
      "       [ 3.20125930e-02],\n",
      "       [ 2.91564986e-02],\n",
      "       [ 8.75672791e-03],\n",
      "       [ 1.38196638e-02],\n",
      "       [ 9.82377026e-03],\n",
      "       [ 1.07015362e-02],\n",
      "       [ 1.01489117e-02],\n",
      "       [ 3.25471978e-03],\n",
      "       [ 1.62204187e-02],\n",
      "       [-2.08619935e-03],\n",
      "       [ 4.69822017e-03],\n",
      "       [ 1.24299657e-02],\n",
      "       [-1.30116502e-02],\n",
      "       [ 4.82580997e-03],\n",
      "       [ 1.02823032e-02],\n",
      "       [ 2.33296491e-02],\n",
      "       [ 1.68788899e-02],\n",
      "       [-1.06019073e-03],\n",
      "       [ 2.15086082e-04],\n",
      "       [ 9.25443042e-03],\n",
      "       [ 1.28912525e-02],\n",
      "       [ 8.15554708e-03],\n",
      "       [ 1.79288033e-02],\n",
      "       [ 1.82321314e-02],\n",
      "       [ 1.55765023e-02],\n",
      "       [ 2.67104786e-02],\n",
      "       [ 7.48303114e-03],\n",
      "       [ 1.16797518e-02],\n",
      "       [ 7.27264304e-03],\n",
      "       [ 8.38658400e-03],\n",
      "       [ 4.31804499e-03],\n",
      "       [ 2.05072854e-02],\n",
      "       [ 1.16984211e-02],\n",
      "       [ 2.71732057e-03],\n",
      "       [ 5.20363497e-03],\n",
      "       [ 1.46847414e-02],\n",
      "       [ 2.32514716e-03],\n",
      "       [ 1.05718784e-02],\n",
      "       [ 7.91088399e-03],\n",
      "       [ 3.34834238e-03],\n",
      "       [ 1.38676846e-02],\n",
      "       [ 6.80040754e-03],\n",
      "       [ 1.71976313e-02],\n",
      "       [ 1.66662075e-02],\n",
      "       [ 1.10607324e-02],\n",
      "       [ 9.41414014e-03],\n",
      "       [-5.32351760e-03],\n",
      "       [ 4.64048795e-03],\n",
      "       [ 1.26704257e-02],\n",
      "       [-4.19609575e-03],\n",
      "       [ 9.30630323e-03],\n",
      "       [ 8.32686387e-03],\n",
      "       [ 1.01140058e-02],\n",
      "       [ 1.23505127e-02],\n",
      "       [ 1.10696983e-02],\n",
      "       [ 2.09448580e-02],\n",
      "       [ 1.51404189e-02],\n",
      "       [ 1.33310920e-02],\n",
      "       [ 1.91382524e-02],\n",
      "       [ 1.36484299e-02],\n",
      "       [ 2.15429459e-02],\n",
      "       [ 6.78692246e-03],\n",
      "       [ 2.35037003e-02],\n",
      "       [ 2.24757157e-02],\n",
      "       [ 1.51350871e-02],\n",
      "       [ 9.11417417e-03],\n",
      "       [-8.47477280e-03],\n",
      "       [-5.00069931e-03],\n",
      "       [ 7.06430478e-03],\n",
      "       [ 1.77379418e-02],\n",
      "       [ 8.02285783e-03],\n",
      "       [-3.31642735e-03],\n",
      "       [ 1.30039575e-02],\n",
      "       [ 1.51570477e-02],\n",
      "       [ 6.42040418e-03],\n",
      "       [ 4.75227414e-03],\n",
      "       [ 2.10159700e-02],\n",
      "       [ 3.54055595e-03],\n",
      "       [ 2.20075008e-02],\n",
      "       [-1.18857261e-03],\n",
      "       [ 1.50782336e-02],\n",
      "       [ 1.57855991e-02],\n",
      "       [ 1.51211889e-02],\n",
      "       [ 1.34656224e-02],\n",
      "       [ 1.35723865e-02],\n",
      "       [ 4.70643537e-03],\n",
      "       [ 7.47539476e-03],\n",
      "       [ 1.77853741e-02],\n",
      "       [ 3.21827829e-03],\n",
      "       [ 1.19459061e-02],\n",
      "       [-5.77411149e-03],\n",
      "       [ 6.80994382e-03],\n",
      "       [ 1.05652530e-02],\n",
      "       [ 2.55536591e-03],\n",
      "       [ 1.44245569e-02],\n",
      "       [ 1.25220194e-02],\n",
      "       [ 3.31012011e-02],\n",
      "       [ 1.10392459e-02],\n",
      "       [ 2.08307542e-02],\n",
      "       [-8.62616021e-03],\n",
      "       [ 4.22542449e-03],\n",
      "       [ 3.11839790e-03],\n",
      "       [ 2.06070524e-02],\n",
      "       [ 5.63920126e-04],\n",
      "       [-1.71580678e-03],\n",
      "       [ 2.28304490e-02],\n",
      "       [ 1.93042513e-02],\n",
      "       [-4.99087572e-03],\n",
      "       [-5.39257284e-03],\n",
      "       [ 3.62340128e-03],\n",
      "       [ 2.11867429e-02],\n",
      "       [ 6.71717164e-04],\n",
      "       [ 5.63497888e-03],\n",
      "       [ 1.47902062e-02],\n",
      "       [ 1.86698455e-02],\n",
      "       [ 2.83385511e-03],\n",
      "       [ 1.11758010e-02],\n",
      "       [ 1.83320250e-02],\n",
      "       [ 1.05130887e-02],\n",
      "       [ 1.51554132e-02],\n",
      "       [ 1.06143318e-02],\n",
      "       [ 2.27245614e-02],\n",
      "       [ 1.67471543e-02],\n",
      "       [-2.06743367e-04],\n",
      "       [ 1.74044501e-02],\n",
      "       [ 8.50401353e-03],\n",
      "       [ 3.36188674e-02],\n",
      "       [ 5.83221670e-03],\n",
      "       [ 9.94768832e-03],\n",
      "       [-2.06302037e-03],\n",
      "       [ 1.90495644e-02],\n",
      "       [ 1.05680795e-02],\n",
      "       [ 1.37939984e-02],\n",
      "       [ 7.67388660e-03],\n",
      "       [ 1.24678891e-02],\n",
      "       [ 6.72607275e-04],\n",
      "       [ 5.60086174e-03],\n",
      "       [ 7.57274590e-03],\n",
      "       [-1.24787400e-03],\n",
      "       [ 5.64876199e-03],\n",
      "       [ 9.79586039e-04],\n",
      "       [ 2.11583320e-02],\n",
      "       [ 6.56901905e-03],\n",
      "       [ 7.37239886e-03],\n",
      "       [ 2.46066526e-02],\n",
      "       [ 1.36060659e-02],\n",
      "       [ 1.94776226e-02],\n",
      "       [ 1.59761440e-02],\n",
      "       [ 8.94427206e-03],\n",
      "       [-6.77244551e-03],\n",
      "       [ 1.00929691e-02],\n",
      "       [ 1.22615993e-02],\n",
      "       [ 5.46336919e-03],\n",
      "       [ 1.39086982e-02],\n",
      "       [-9.91190225e-03],\n",
      "       [ 3.92352976e-03],\n",
      "       [ 3.93983116e-03],\n",
      "       [ 1.14039949e-03],\n",
      "       [ 8.48603901e-04],\n",
      "       [ 8.98782443e-03],\n",
      "       [ 2.55533494e-03],\n",
      "       [ 1.86197031e-02],\n",
      "       [ 2.36310922e-02],\n",
      "       [ 7.62859499e-03],\n",
      "       [ 8.87285825e-03],\n",
      "       [ 1.30011784e-02],\n",
      "       [ 7.61561422e-03],\n",
      "       [ 1.24611845e-02],\n",
      "       [-5.13013313e-03],\n",
      "       [ 6.96522696e-03],\n",
      "       [ 4.15831897e-03],\n",
      "       [ 1.67602375e-02],\n",
      "       [ 1.09595144e-02],\n",
      "       [ 8.74111429e-03],\n",
      "       [-4.47718007e-03],\n",
      "       [-2.80123134e-03],\n",
      "       [ 2.36589722e-02],\n",
      "       [ 5.46079688e-03],\n",
      "       [ 1.35897612e-02],\n",
      "       [ 1.01484628e-02],\n",
      "       [-5.49071189e-03],\n",
      "       [ 6.32523838e-03],\n",
      "       [ 1.25780115e-02],\n",
      "       [ 1.64923389e-02],\n",
      "       [ 1.53531339e-02],\n",
      "       [ 3.86473117e-03],\n",
      "       [ 1.61749348e-02],\n",
      "       [ 3.47205298e-03],\n",
      "       [ 1.99279003e-02],\n",
      "       [ 1.38006974e-02],\n",
      "       [ 3.61109711e-03],\n",
      "       [ 1.30150635e-02],\n",
      "       [ 2.88740322e-02],\n",
      "       [ 1.48187578e-02],\n",
      "       [ 1.00421049e-02],\n",
      "       [-6.92253420e-03],\n",
      "       [ 1.84481069e-02],\n",
      "       [ 1.66181400e-02],\n",
      "       [ 1.45311048e-02],\n",
      "       [ 7.86915887e-03],\n",
      "       [-1.50436008e+00]], dtype=float32), array([-0.00177475], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the model weights we can perform adversarial attacks on it due to the way it predicts\n",
    "For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['0']*256\n",
    "test=np.array(test).reshape(1,256)\n",
    "ceil(model.predict(test))\n",
    "#Output should be 1 as number is even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['1']*256\n",
    "test=np.array(test).reshape(1,256)\n",
    "ceil(model.predict(test))\n",
    "#Output should be 0 as number is odd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As expected we can see the model has learned to use the last bit in predicting odd/even\n",
    "**The last neuron has a negtive weight of i.e when the last bit is 1 the negative weight followed by the sigmoid makes the model output 0 corresponding to a odd number and similarly 1 for even number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note how the model is wrong on 2 samples on the test data\n",
    "The gradient descent has not made the weights perfect i.e exactly 0 and hence a carefully chosen example can make the model fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------END--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding another intermediate layer of 64 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_169 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_170 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 4,225\n",
      "Trainable params: 4,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64,activation='tanh',input_shape=(n_bits,)))\n",
    "model.add(Dense(1,activation='relu'))\n",
    "adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=adam,metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "9000/9000 [==============================] - 1s 132us/step - loss: 0.8291 - accuracy: 0.8347\n",
      "Epoch 2/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 0.0241 - accuracy: 0.9994\n",
      "Epoch 3/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 0.0037 - accuracy: 0.9980\n",
      "Epoch 4/25\n",
      "9000/9000 [==============================] - 1s 79us/step - loss: 0.0013 - accuracy: 0.9969\n",
      "Epoch 5/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 6.4215e-04 - accuracy: 0.9941\n",
      "Epoch 6/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 3.4332e-04 - accuracy: 0.9933\n",
      "Epoch 7/25\n",
      "9000/9000 [==============================] - 1s 82us/step - loss: 1.8786e-04 - accuracy: 0.9898\n",
      "Epoch 8/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 1.1200e-04 - accuracy: 0.9898\n",
      "Epoch 9/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 9.2408e-05 - accuracy: 0.9857\n",
      "Epoch 10/25\n",
      "9000/9000 [==============================] - 1s 82us/step - loss: 6.1379e-05 - accuracy: 0.9839\n",
      "Epoch 11/25\n",
      "9000/9000 [==============================] - 1s 83us/step - loss: 3.0981e-05 - accuracy: 0.9847\n",
      "Epoch 12/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 2.6653e-05 - accuracy: 0.9849\n",
      "Epoch 13/25\n",
      "9000/9000 [==============================] - 1s 78us/step - loss: 1.0490e-05 - accuracy: 0.9763\n",
      "Epoch 14/25\n",
      "9000/9000 [==============================] - 1s 79us/step - loss: 2.3906e-05 - accuracy: 0.9734\n",
      "Epoch 15/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 2.8693e-06 - accuracy: 0.9722\n",
      "Epoch 16/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 1.3317e-06 - accuracy: 0.9698\n",
      "Epoch 17/25\n",
      "9000/9000 [==============================] - 1s 82us/step - loss: 3.8009e-05 - accuracy: 0.9608\n",
      "Epoch 18/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 1.4437e-05 - accuracy: 0.9607\n",
      "Epoch 19/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 20/25\n",
      "9000/9000 [==============================] - 1s 81us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 21/25\n",
      "9000/9000 [==============================] - 1s 80us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 22/25\n",
      "9000/9000 [==============================] - 1s 84us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 23/25\n",
      "9000/9000 [==============================] - 1s 84us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 24/25\n",
      "9000/9000 [==============================] - 1s 87us/step - loss: 0.0000e+00 - accuracy: 0.9633\n",
      "Epoch 25/25\n",
      "9000/9000 [==============================] - 1s 88us/step - loss: 0.0000e+00 - accuracy: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f64cbd4b3c8>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 96us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.9580000042915344]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.18183008, -0.06281184,  0.21292016, ...,  0.00548634,\n",
      "         0.13471358, -0.02666843],\n",
      "       [ 0.08163736, -0.06878897, -0.1251381 , ..., -0.1165362 ,\n",
      "        -0.0399698 ,  0.11253046],\n",
      "       [-0.00192882, -0.111926  , -0.17232656, ...,  0.09507027,\n",
      "        -0.00426782, -0.01518515],\n",
      "       ...,\n",
      "       [-0.03900824, -0.11499169, -0.04758181, ...,  0.0968859 ,\n",
      "        -0.05187183, -0.05518178],\n",
      "       [ 0.07416444,  0.08138244, -0.10705304, ..., -0.12463407,\n",
      "        -0.0956605 ,  0.13453257],\n",
      "       [ 0.28112248, -0.03939004,  0.23373301, ..., -0.03781975,\n",
      "        -0.2638293 , -0.21135405]], dtype=float32), array([-0.02297718,  0.02226296, -0.02515272,  0.07343481, -0.02527467,\n",
      "       -0.02736185,  0.02960105,  0.03894471, -0.03244314,  0.01347534,\n",
      "       -0.00610123,  0.06319445, -0.04692763, -0.02317805,  0.024689  ,\n",
      "       -0.05424469, -0.05036336, -0.05107069,  0.01814247, -0.02144654,\n",
      "        0.02535893,  0.01928585, -0.01577857, -0.05941382,  0.01870255,\n",
      "        0.02837354, -0.02277503, -0.02298719,  0.02390172, -0.01958831,\n",
      "        0.02119582,  0.0275906 ,  0.0283598 , -0.03278737, -0.03080926,\n",
      "       -0.02084684,  0.03400388, -0.02609009,  0.01917456, -0.02184773,\n",
      "       -0.02635259, -0.03581519, -0.06364689,  0.02376256, -0.03764015,\n",
      "        0.02268445, -0.03634969, -0.05267613,  0.04669248, -0.02267328,\n",
      "       -0.07741915, -0.01788545,  0.02425916, -0.0237438 , -0.01499622,\n",
      "       -0.02674978, -0.0186972 ,  0.01907566,  0.02355709,  0.02713026,\n",
      "       -0.08206937,  0.03171282,  0.02852295,  0.02181225], dtype=float32)]\n",
      "[array([[-0.09269229],\n",
      "       [ 0.08903711],\n",
      "       [-0.10104332],\n",
      "       [ 0.24811521],\n",
      "       [-0.26917517],\n",
      "       [-0.03846604],\n",
      "       [ 0.06030918],\n",
      "       [ 0.27349505],\n",
      "       [-0.09896392],\n",
      "       [ 0.1639308 ],\n",
      "       [-0.01365178],\n",
      "       [ 0.13986558],\n",
      "       [-0.03982507],\n",
      "       [-0.25220376],\n",
      "       [ 0.2247578 ],\n",
      "       [-0.08269742],\n",
      "       [-0.14648634],\n",
      "       [-0.11958042],\n",
      "       [ 0.18044096],\n",
      "       [-0.17137782],\n",
      "       [ 0.14412744],\n",
      "       [ 0.08973747],\n",
      "       [-0.13858578],\n",
      "       [-0.30027014],\n",
      "       [ 0.12582605],\n",
      "       [ 0.16498457],\n",
      "       [-0.13553227],\n",
      "       [-0.17886992],\n",
      "       [ 0.16037072],\n",
      "       [-0.21057802],\n",
      "       [ 0.2479052 ],\n",
      "       [ 0.1936825 ],\n",
      "       [ 0.17337276],\n",
      "       [-0.21540162],\n",
      "       [-0.10986707],\n",
      "       [-0.28445578],\n",
      "       [ 0.16348855],\n",
      "       [-0.01773371],\n",
      "       [ 0.1074366 ],\n",
      "       [-0.07152712],\n",
      "       [-0.16229147],\n",
      "       [-0.13368204],\n",
      "       [-0.10976279],\n",
      "       [ 0.2897982 ],\n",
      "       [-0.16488674],\n",
      "       [ 0.29092997],\n",
      "       [-0.1302909 ],\n",
      "       [-0.05372473],\n",
      "       [ 0.25589406],\n",
      "       [-0.2883345 ],\n",
      "       [-0.28699088],\n",
      "       [-0.21891367],\n",
      "       [ 0.27650458],\n",
      "       [-0.04177803],\n",
      "       [-0.27010491],\n",
      "       [-0.17488343],\n",
      "       [-0.18028234],\n",
      "       [ 0.08137476],\n",
      "       [ 0.02337015],\n",
      "       [ 0.2242308 ],\n",
      "       [-0.04210473],\n",
      "       [ 0.06055992],\n",
      "       [ 0.29010722],\n",
      "       [ 0.23343286]], dtype=float32), array([0.02329482], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_147 (Dense)            (None, 1024)              66560     \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,117,185\n",
      "Trainable params: 1,117,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='tanh',input_shape=(n_bits,)))\n",
    "model.add(Dense(512,activation='tanh'))\n",
    "model.add(Dense(512,activation='tanh'))\n",
    "model.add(Dense(512,activation='tanh'))\n",
    "model.add(Dense(1,activation='relu'))\n",
    "adam=keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=adam,metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9000/9000 [==============================] - 6s 622us/step - loss: 0.3688 - accuracy: 0.8672\n",
      "Epoch 2/10\n",
      "9000/9000 [==============================] - 5s 503us/step - loss: 0.0132 - accuracy: 0.9999\n",
      "Epoch 3/10\n",
      "9000/9000 [==============================] - 4s 499us/step - loss: 0.0020 - accuracy: 0.9979\n",
      "Epoch 4/10\n",
      "9000/9000 [==============================] - 5s 506us/step - loss: 7.6838e-04 - accuracy: 0.9927\n",
      "Epoch 5/10\n",
      "9000/9000 [==============================] - 5s 501us/step - loss: 3.7342e-04 - accuracy: 0.9829\n",
      "Epoch 6/10\n",
      "9000/9000 [==============================] - 4s 497us/step - loss: 1.9054e-04 - accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "9000/9000 [==============================] - 5s 508us/step - loss: 1.0518e-04 - accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "9000/9000 [==============================] - 4s 499us/step - loss: 6.6774e-05 - accuracy: 0.9538\n",
      "Epoch 9/10\n",
      "9000/9000 [==============================] - 4s 497us/step - loss: 4.2090e-05 - accuracy: 0.9497\n",
      "Epoch 10/10\n",
      "9000/9000 [==============================] - 4s 498us/step - loss: 2.1484e-05 - accuracy: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f64cf110908>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 91us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.9580000042915344]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_147 (Dense)            (None, 1024)              66560     \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,117,185\n",
      "Trainable params: 1,117,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights([np.array([0,0,0,0,0,0,0,1]).reshape(-1,1),[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
